{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title.png](https://i.ibb.co/2KmT38V/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![objectives.png](https://i.ibb.co/fxbWnNQ/objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!   pip install -qU \\\n",
    "    openai==1.30 \\\n",
    "    pinecone-client==4.1.0 \\\n",
    "    datasets==2.19 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def chunk_display_html(chunk: Dict[str, str]) -> str:\n",
    "    html_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table {{\n",
    "        font-family: arial, sans-serif;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    td, th {{\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Key</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Title</td>\n",
    "            <td>{title}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>doi</td>\n",
    "            <td>{doi}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk ID</td>\n",
    "            <td>{chunk_id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>chunk</td>\n",
    "            <td>{chunk}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>id</td>\n",
    "            <td>{id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Summary</td>\n",
    "            <td>{summary}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Source</td>\n",
    "            <td>{source}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Authors</td>\n",
    "            <td>{authors}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>{categories}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comment</td>\n",
    "            <td>{comment}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Journal Reference</td>\n",
    "            <td>{journal_ref}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Primary Category</td>\n",
    "            <td>{primary_category}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Published</td>\n",
    "            <td>{published}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Updated</td>\n",
    "            <td>{updated}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>References</td>\n",
    "            <td>{references}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # Format the HTML with the generated rows\n",
    "    html_output = html_template.format(\n",
    "        doi=chunk.get(\"doi\", \"N/A\"),\n",
    "        chunk_id=chunk.get(\"chunk-id\", \"N/A\"),\n",
    "        chunk=chunk.get(\"chunk\", \"N/A\"),\n",
    "        id=chunk.get(\"id\", \"N/A\"),\n",
    "        title=chunk.get(\"title\", \"N/A\"),\n",
    "        summary=chunk.get(\"summary\", \"N/A\"),\n",
    "        source=chunk.get(\"source\", \"N/A\"),\n",
    "        authors=chunk.get(\"authors\", \"N/A\"),\n",
    "        categories=chunk.get(\"categories\", \"N/A\"),\n",
    "        comment=chunk.get(\"comment\", \"N/A\"),\n",
    "        journal_ref=chunk.get(\"journal_ref\", \"N/A\"),\n",
    "        primary_category=chunk.get(\"primary_category\", \"N/A\"),\n",
    "        published=chunk.get(\"published\", \"N/A\"),\n",
    "        updated=chunk.get(\"updated\", \"N/A\"),\n",
    "        references=chunk.get(\"references\", \"N/A\"),\n",
    "    )\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_retrieved_context(context_response):\n",
    "    # HTML template for the main container and individual tables\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            margin: 10px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #dddddd;\n",
    "        }}\n",
    "        table {{\n",
    "            font-family: arial, sans-serif;\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        td, th {{\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            {tables}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate HTML table for a single dictionary\n",
    "    def generate_table_for_dict(data):\n",
    "        rows = \"\\n\".join(\n",
    "            \"<tr><td>{key}</td><td>{value}</td></tr>\".format(\n",
    "                key=key, value=value if value is not None else \"N/A\"\n",
    "            )\n",
    "            for key, value in data.items()\n",
    "        )\n",
    "        table_html = \"\"\"\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Key</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                {rows}\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\".format(\n",
    "            rows=rows\n",
    "        )\n",
    "        return table_html\n",
    "\n",
    "    # Generate HTML tables for all dictionaries in the list\n",
    "    tables = \"\\n\".join(\n",
    "        generate_table_for_dict(data[\"metadata\"]) for data in context_response\n",
    "    )\n",
    "\n",
    "    # Format the main HTML with the generated tables\n",
    "    html_output = html_template.format(tables=tables)\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_markdown(content: str) -> None:\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-away-rag.png](https://i.ibb.co/Y288TWR/step-away-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the OpenAI API key and instantiate the OpenAI clinet.\n",
    "\n",
    "Copy this openai API key `sk-XXXXXXXXXX` into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement request to OpenAI GPT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement a function that will send a prompt to an LLM and return an answer.  \n",
    "The OpenAI client has the following signature:  \n",
    "```python \n",
    "openai_client.chat.completions.create(model: str, messages=List[Dict[str, str]])\n",
    "```\n",
    "\n",
    "API reference: https://platform.openai.com/docs/api-reference/chat/create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_completion(prompt: str, openai_client: OpenAI, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output is markdown\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(llm_completion(\"What is the capital of Germany?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(llm_completion(\"Who is 25th person that landed on the moon?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hallucinations-problem.png](https://i.ibb.co/gMvNZC6/hallucinations-problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(llm_completion(\"What are key benefits of mistral 7B?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge-cutoff.png](https://i.ibb.co/ccccpxZ/knowledge-cutoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_example = \"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "\n",
    "Context:\n",
    "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "\n",
    "Question: What are key benefits of Mistral 7B?\n",
    "\"\"\"\n",
    "display_markdown(llm_completion(context_example, openai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![why-rag.png](https://i.ibb.co/KF3xr64/why-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how-to-build-rag.png](https://i.ibb.co/wgXqfFv/how-to-build-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![build-knowledge-base.png](https://i.ibb.co/dGnjrCk/build-knowledge-base.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone \n",
    "\n",
    "Enter the Pinecone API key inside the prompt and create a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_REGION = \"eu-west-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "INDEX_NAME = \"pinecone-workshop-1\"\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index\n",
    "\n",
    "Create a Serverless Index with OpenAI embeddings size with cosine similarity metrics. \n",
    "The index creation requires index name, dimension of embeddings, similarity metric and serverless spec for a serverless setup.  \n",
    "\n",
    "Pseudo code:  \n",
    "```python\n",
    "pinecone.create_index(\n",
    "    name=<add-index-name>,\n",
    "    dimension=<add-vector-dimension>,\n",
    "    metric=<add-similarity-metric>,\n",
    "    spec=ServerlessSpec(cloud=<add-cloud-name>, region=<add-region-name>)\n",
    ")\n",
    "```\n",
    "\n",
    "More info o serverless: https://docs.pinecone.io/reference/architecture/serverless-architecture#overview  \n",
    "Ref API: https://docs.pinecone.io/reference/api/control-plane/create_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Check if the index already exists and delete it\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# TODO: Create a new index with the specified name, dimension, metric, and spec\n",
    "# Use `pinecone.create_index(name=, dimension=, metric=, spec=ServerlessSpec(cloud=, region=)`\n",
    "\n",
    "# Create a new Index reference object with the specified name and pool_threads\n",
    "index = pinecone.Index(INDEX_NAME, pool_threads=20)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use a sample of 1000 AI papers that can be found here: https://huggingface.co/datasets/smartcat/ai-arxiv2-chunks-embedded \n",
    "\n",
    "The data set is already chunked and encoded using `text-embeddings-3-small` so we can just load and upsert it to the Pinecone.\n",
    "\n",
    "If you want to play with chunking strategies and embeddings, you can find the full data set here: https://huggingface.co/datasets/jamescalam/ai-arxiv2\n",
    "\n",
    "Dataset API reference: https://huggingface.co/docs/datasets/en/index  \n",
    "Slicing and indexing: https://huggingface.co/docs/datasets/en/access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"smartcat/ai-arxiv2-chunks-embedded\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_display_html(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset[0][\"embeddings\"]))\n",
    "print(dataset[0][\"embeddings\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset[0][\"metadata\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data upsert to Pinecone\n",
    "\n",
    "Let insert data to the Pinecone in batches. \n",
    "\n",
    "From our data set we need 3 columns:\n",
    "1. `id` - the ID of the chunk we want to insert\n",
    "2. `embeddings` - contains a vector embedding of the chunk. It uses `text-embeddings-3-small`\n",
    "3. `metadata` - a dictinary with additional data about the chunk. \n",
    "\n",
    "The code for upserting:\n",
    "```python\n",
    "index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    " ])\n",
    "```\n",
    "\n",
    "### Note on optimization\n",
    "\n",
    "To improve througput, we are going to add `async_req=True` parameter. \n",
    "Upsert will return futures that we need to wait.\n",
    "\n",
    "Optimization tips:\n",
    "1. Deploy application at the same region\n",
    "2. Use batching upsert\n",
    "3. Use parallelized upsert\n",
    "4. Use GRPCIndex, but make sure to add backoff for throttling\n",
    "5. Use namespaces and metadata filtering\n",
    "6. Avoid quotas and limits: https://docs.pinecone.io/reference/quotas-and-limits\n",
    "\n",
    "Optimized upsert code:\n",
    "```python\n",
    "future = index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    "    ],\n",
    "    async_req=True\n",
    ")\n",
    "....\n",
    "future.get() # wait for upsert to complete\n",
    "```\n",
    "\n",
    "For scale-up and optimizations make sure to read: : https://docs.pinecone.io/guides/operations/performance-tuning#increasing-throughput  \n",
    "Metadata filtering: Metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pinecone import Index\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 100) -> None:\n",
    "    futures = []\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        i_end = min(i + batch_size, len(ds))\n",
    "        batch = ds.select(range(i, i_end))\n",
    "\n",
    "        # The upsert requires the vectors to be a list of tuples (id, vector, metadata)\n",
    "        # Example: [(id1, vector1, metadata1), (id2, vector2, metadata2), ...]\n",
    "        # TODO: Select the id, embeddings, and metadata from the batch\n",
    "        ...\n",
    "\n",
    "        # TODO: Upsert the vectors to the Pinecone index asynchronously\n",
    "        # Use `index.upsert(vectors=[(id1, vector1, metadata1), (id2, vector2, metadata2)], async_req=True)`\n",
    "        futures.append(\n",
    "           ...\n",
    "        )\n",
    "\n",
    "    # Wait for all the upserts to complete\n",
    "    for future in tqdm(futures, total=len(futures), desc=\"Upsert to Pinecone\"):\n",
    "        future.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_batch(dataset.select(range(500)), index, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_batch(dataset.select(range(40_000)), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has been done with data set?\n",
    "\n",
    "![chunking-dataset.png](https://i.ibb.co/9wV70Q7/chunking-dataset.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "#### Chunking Strategies\n",
    "1. Character split (with overlapping)\n",
    "2. Recursive character split\n",
    "3. Document specific splitting\n",
    "4. Semantic Chunking\n",
    "5. Agentic?\n",
    "6. More?\n",
    "\n",
    "Introduction to chunking: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![done-next.png](https://i.ibb.co/QYC9nrb/done-next.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve against the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![semantic-search.png](https://i.ibb.co/2dWGRHn/semantic-search.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we inserted everythong to Pinecone, let's query it. The input is query text and the output is the top similar chunks.\n",
    "Steps: \n",
    "1. Encode the input text to generate embeddings \n",
    "2. Call Pinecone's query function to retrieve top K similar results\n",
    "\n",
    "### Encode (create embeddings)\n",
    "\n",
    "Create embeddings:\n",
    "```python\n",
    "openai_client.embeddings.create(model=\"embedding-model-name\", input=\"Text to create embeddings\")\n",
    "```\n",
    "\n",
    "Embedding API Ref: https://platform.openai.com/docs/api-reference/embeddings  \n",
    "Query API Ref: https://docs.pinecone.io/reference/api/data-plane/query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def encode(\n",
    "    text: str, openai_client: OpenAI, model: str = \"text-embedding-3-small\"\n",
    ") -> List[float]:\n",
    "    # TODO: create embeddings using OpenAI API\n",
    "    # 1. Call openai_client.embeddings.create with the model and text\n",
    "    response = ...\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = encode(\"What are key benefits of mistral 7B?\", openai)\n",
    "print(res[:10])\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Pinecone\n",
    "\n",
    "Query Pinecone:\n",
    "We are going to retrieve metadata, but not vectors itself\n",
    "```python\n",
    "index.query(\n",
    "        vector=QUERY_EMBEDDINGS, top_k=TOP_K, include_metadata=True, include_values=False\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import QueryResponse\n",
    "\n",
    "\n",
    "def semantic_search(\n",
    "    query: str, index: Index, openai_client: OpenAI, top_k: int = 10\n",
    ") -> QueryResponse:\n",
    "    # TODO: Implement semantic search using the OpenAI API and Pinecone index\n",
    "    # 1. Reuse `encode` function to create embeddings for the query\n",
    "    # 2. Use `index.query` to query the index with the vector and top_k and set include_metadata=True\n",
    "    query_embedding = ...\n",
    "    ret = index.query( ... )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = semantic_search(\"What is Mistral 7B?\", index, openai)\n",
    "print(res.matches[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augment and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![workflow-rag-simple.png](https://i.ibb.co/p0gwY23/workflow-rag-simple.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final response\n",
    "\n",
    "Combine all pieces together:\n",
    "1. Perform a semantic search on input query\n",
    "2. Build a context (prompt) for a LLM\n",
    "3. Call LLM to generate a final response\n",
    "4. Return a final response and retrieved context\n",
    "\n",
    "The relevant context can be found in metadata, you can use:\n",
    "1. `title` - a Paper title\n",
    "2. `published` - a publish date\n",
    "3. `primart_category` \n",
    "4. `summary` - a paper summary\n",
    "5. `text` - a chunk text - this is the most useful info to build a context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def from_metadata(metadata: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "***\n",
    "    Title: {metadata['title']}\n",
    "    Authors: {metadata['authors'][:5]}\n",
    "    Published: {metadata['published']}\n",
    "    Primary category: {metadata['primary_category']}\n",
    "    Paper summary: {metadata['summary']}\n",
    "    Text: {metadata['text']}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(query: str, query_results: QueryResponse) -> str:\n",
    "    context = \"\\n\".join(\n",
    "        [from_metadata(result.metadata) for result in query_results.matches]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "The answers should be clear, easy to understand, complete and comprehensive.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rag(query: str, index: Index, openai_client: OpenAI, top_k: int = 5) -> Tuple[str, QueryResponse]:\n",
    "    # TODO: Wire all RAG pieces together to generate a response (retrieve, augment, generate)\n",
    "    # 1. [RETRIEVE]: Reuse `semantic_search` function to get the top_k results\n",
    "    # 2. [AUGMENT]: Use `get_prompt` function to generate the prompt (context + question)\n",
    "    # 3. [GENERATE]: Use `llm_completion` function to generate the response\n",
    "    # 4. Return the response and query_results\n",
    "    query_results = ...\n",
    "    prompt = ...\n",
    "    response = ...\n",
    "    return response, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, context = rag(\"What are key benefits of Mistral 7B?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, _ = rag(\"The difference between Mistral and Kosmos?\", index, openai, top_k=20)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Topics\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "TBA - Add mini-agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Once we explored the simple components of RAG, we can use existing frameworks to build them.  \n",
    "\n",
    "Langchain: https://www.langchain.com/langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index, embedding=embeddings, text_key=\"text\", pinecone_api_key=PINECONE_API_KEY, index_name=INDEX_NAME,)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "langchain_rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_rag.invoke(\"What are all benefits of using Mistral 7B?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune vs RAG vs Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag-vs-prompt-vs-finetune.png](https://i.ibb.co/C8rvnTY/Screenshot-2024-05-30-at-21-17-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, context = rag(\n",
    "    \"The key difference between Mistral 7B and Palm?\", index, openai, top_k=20\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Time\n",
    "\n",
    "#### Query Time\n",
    "\n",
    "#### Evaluations\n",
    "\n",
    "#### Agentic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Better embeddings\n",
    "# Better chunking\n",
    "# Better indexing\n",
    "# Better query understanding\n",
    "# History management\n",
    "# Evaluation\n",
    "# Ranking (Context Limitation)\n",
    "# Guardrails\n",
    "# Frameworks (langchain / canopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
