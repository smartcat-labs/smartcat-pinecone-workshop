{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title.png](https://i.ibb.co/2KmT38V/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![objectives.png](https://i.ibb.co/fxbWnNQ/objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!   pip install -qU \\\n",
    "    openai==1.30 \\\n",
    "    \"pinecone-client[grpc]==4.1.0\" \\\n",
    "    datasets==2.19 \\\n",
    "    pandas==2.2.2 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def chunk_display_html(chunk: Dict[str, str]) -> str:\n",
    "    html_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table {{\n",
    "        font-family: arial, sans-serif;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    td, th {{\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Key</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Title</td>\n",
    "            <td>{title}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>doi</td>\n",
    "            <td>{doi}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk ID</td>\n",
    "            <td>{chunk_id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>chunk</td>\n",
    "            <td>{chunk}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>id</td>\n",
    "            <td>{id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Summary</td>\n",
    "            <td>{summary}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Source</td>\n",
    "            <td>{source}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Authors</td>\n",
    "            <td>{authors}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>{categories}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comment</td>\n",
    "            <td>{comment}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Journal Reference</td>\n",
    "            <td>{journal_ref}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Primary Category</td>\n",
    "            <td>{primary_category}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Published</td>\n",
    "            <td>{published}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Updated</td>\n",
    "            <td>{updated}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>References</td>\n",
    "            <td>{references}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # Format the HTML with the generated rows\n",
    "    html_output = html_template.format(\n",
    "        doi=chunk.get(\"doi\", \"N/A\"),\n",
    "        chunk_id=chunk.get(\"chunk-id\", \"N/A\"),\n",
    "        chunk=chunk.get(\"chunk\", \"N/A\"),\n",
    "        id=chunk.get(\"id\", \"N/A\"),\n",
    "        title=chunk.get(\"title\", \"N/A\"),\n",
    "        summary=chunk.get(\"summary\", \"N/A\"),\n",
    "        source=chunk.get(\"source\", \"N/A\"),\n",
    "        authors=chunk.get(\"authors\", \"N/A\"),\n",
    "        categories=chunk.get(\"categories\", \"N/A\"),\n",
    "        comment=chunk.get(\"comment\", \"N/A\"),\n",
    "        journal_ref=chunk.get(\"journal_ref\", \"N/A\"),\n",
    "        primary_category=chunk.get(\"primary_category\", \"N/A\"),\n",
    "        published=chunk.get(\"published\", \"N/A\"),\n",
    "        updated=chunk.get(\"updated\", \"N/A\"),\n",
    "        references=chunk.get(\"references\", \"N/A\"),\n",
    "    )\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_retrieved_context(context_response):\n",
    "    # HTML template for the main container and individual tables\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            margin: 10px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #dddddd;\n",
    "        }}\n",
    "        table {{\n",
    "            font-family: arial, sans-serif;\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        td, th {{\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            {tables}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate HTML table for a single dictionary\n",
    "    def generate_table_for_dict(data):\n",
    "        rows = \"\\n\".join(\n",
    "            \"<tr><td>{key}</td><td>{value}</td></tr>\".format(\n",
    "                key=key, value=value if value is not None else \"N/A\"\n",
    "            )\n",
    "            for key, value in data.items()\n",
    "        )\n",
    "        table_html = \"\"\"\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Key</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                {rows}\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\".format(\n",
    "            rows=rows\n",
    "        )\n",
    "        return table_html\n",
    "\n",
    "    # Generate HTML tables for all dictionaries in the list\n",
    "    tables = \"\\n\".join(\n",
    "        generate_table_for_dict(data[\"metadata\"]) for data in context_response\n",
    "    )\n",
    "\n",
    "    # Format the main HTML with the generated tables\n",
    "    html_output = html_template.format(tables=tables)\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_markdown(content: str) -> None:\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-away-rag.png](https://i.ibb.co/Y288TWR/step-away-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the OpenAI API key and instantiate the OpenAI clinet.\n",
    "\n",
    "Copy this openai API key into the prompt.\n",
    "The key is shared here: https://docs.google.com/document/d/1B-UWsusJOjj7n96_3GXUW5iXSn4ErXjxNq--aapP_NI/edit?pli=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text using OpenAI GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, openai_client: OpenAI, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    # API reference: https://platform.openai.com/docs/api-reference/chat/create\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output is markdown\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of Germany is Berlin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"What is the capital of Germany?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 25th person to land on the moon was Harrison Schmitt. He was part of the Apollo 17 mission in December 1972 and was the only geologist to walk on the moon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"Who is 25th person that landed on the moon?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hallucinations-problem.png](https://i.ibb.co/gMvNZC6/hallucinations-problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Mistral 7B is a powerful and versatile wind turbine that offers several key benefits:\n",
       "\n",
       "1. **High Efficiency**: The Mistral 7B is designed to maximize energy production with its high-efficiency blades and generator, ensuring optimal performance in various wind conditions.\n",
       "\n",
       "2. **Reliability**: This wind turbine is built with durable materials and a robust design, making it reliable and able to withstand harsh weather conditions.\n",
       "\n",
       "3. **Low Maintenance**: The Mistral 7B requires minimal maintenance, reducing operational costs and ensuring consistent performance over its lifespan.\n",
       "\n",
       "4. **Quiet Operation**: With advanced noise reduction technology, this wind turbine operates quietly, making it suitable for residential and commercial applications.\n",
       "\n",
       "5. **Scalability**: The Mistral 7B can be easily integrated into existing renewable energy systems or used as a standalone solution, providing flexibility for different energy needs.\n",
       "\n",
       "Overall, the Mistral 7B offers a combination of efficiency, reliability, low maintenance, quiet operation, and scalability, making it a valuable choice for renewable energy generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"What are key benefits of mistral 7B?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge-cutoff.png](https://i.ibb.co/ccccpxZ/knowledge-cutoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B include:\n",
       "\n",
       "1. Superior performance and efficiency compared to other models.\n",
       "2. Outperforming the best open 13B model (Llama 2) across all evaluated benchmarks.\n",
       "3. Outperforming the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.\n",
       "4. Leveraging grouped-query attention (GQA) for faster inference.\n",
       "5. Using sliding window attention (SWA) to effectively handle sequences of arbitrary length with reduced inference cost.\n",
       "6. Providing a fine-tuned model, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_example = \"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "\n",
    "Context:\n",
    "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "\n",
    "Question: What are key benefits of Mistral 7B?\n",
    "\"\"\"\n",
    "display_markdown(generate(context_example, openai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![why-rag.png](https://i.ibb.co/KF3xr64/why-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how-to-build-rag.png](https://i.ibb.co/wgXqfFv/how-to-build-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![build-knowledge-base.png](https://i.ibb.co/dGnjrCk/build-knowledge-base.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone \n",
    "\n",
    "Enter the Pinecone API key inside the prompt and create a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_REGION = \"us-east-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "INDEX_NAME = \"pinecone-workshop-1\"\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'pinecone-worshop-1-2kw20wn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-worshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'pinecone-workshop-1-2kw20wn.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-workshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC\n",
    "#from pinecone import Pinecone\n",
    "\n",
    "pinecone = PineconeGRPC(api_key=PINECONE_API_KEY)\n",
    "# pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index\n",
    "\n",
    "More info o serverless: https://docs.pinecone.io/reference/architecture/serverless-architecture#overview  \n",
    "Ref API: https://docs.pinecone.io/reference/api/control-plane/create_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0}},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Check if the index already exists and delete it\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# Create a new index with the specified name, dimension, metric, and spec\n",
    "# Docs: https://docs.pinecone.io/reference/api/control-plane/create_index\n",
    "pinecone.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=VECTOR_DIMENSIONS,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
    ")\n",
    "\n",
    "# Create a new Index reference object with the specified name\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use a sample of 1000 AI papers that can be found here: https://huggingface.co/datasets/smartcat/ai-arxiv2-chunks-embedded \n",
    "\n",
    "The data set is already chunked and encoded using `text-embeddings-3-small` so we can just load and upsert it to the Pinecone.\n",
    "\n",
    "If you want to play with chunking strategies and embeddings, you can find the full data set here: https://huggingface.co/datasets/jamescalam/ai-arxiv2\n",
    "\n",
    "Dataset API reference: https://huggingface.co/docs/datasets/en/index  \n",
    "Slicing and indexing: https://huggingface.co/docs/datasets/en/access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'embeddings', 'metadata'],\n",
       "    num_rows: 79782\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"smartcat/ai-arxiv2-chunks-embedded\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "    table {\n",
       "        font-family: arial, sans-serif;\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    td, th {\n",
       "        border: 1px solid #dddddd;\n",
       "        text-align: left;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Key</th>\n",
       "            <th>Value</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Title</td>\n",
       "            <td>Foundations of Vector Retrieval</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>doi</td>\n",
       "            <td>2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk ID</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>chunk</td>\n",
       "            <td>4 2 0 2\n",
       "n a J 7 1 ] S D . s c [\n",
       "1 v 0 5 3 9 0 . 1 0 4 2 : v i X r a\n",
       "Sebastian Bruch\n",
       "# Foundations of Vector Retrieval\n",
       "# Preface\n",
       "We are witness to a few years of remarkable developments in Artificial Intelligence with the use of advanced machine learning algorithms, and in particular, deep learning. Gargantuan, complex neural networks that can learn through self-supervisionâand quickly so with the aid of special- ized hardwareâtransformed the research landscape so dramatically that, incremental overnight it seems, many fields experienced not the usual, progress, but rather a leap forward. Machine translation, natural language understanding, information retrieval, recommender systems, and computer vision are but a few examples of research areas that have had to grapple with the shock. Countless other disciplines beyond computer science such as robotics, biology, and chemistry too have benefited from deep learning.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>id</td>\n",
       "            <td>2401.09350#0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Summary</td>\n",
       "            <td>Vectors are universal mathematical objects that can represent text, images,\n",
       "speech, or a mix of these data modalities. That happens regardless of whether\n",
       "data is represented by hand-crafted features or learnt embeddings. Collect a\n",
       "large enough quantity of such vectors and the question of retrieval becomes\n",
       "urgently relevant: Finding vectors that are more similar to a query vector.\n",
       "This monograph is concerned with the question above and covers fundamental\n",
       "concepts along with advanced data structures and algorithms for vector\n",
       "retrieval. In doing so, it recaps this fascinating topic and lowers barriers of\n",
       "entry into this rich area of research.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Source</td>\n",
       "            <td>http://arxiv.org/pdf/2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Authors</td>\n",
       "            <td>Sebastian Bruch</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Categories</td>\n",
       "            <td>cs.DS, cs.IR</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Comment</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Journal Reference</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Primary Category</td>\n",
       "            <td>cs.DS</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Published</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Updated</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>References</td>\n",
       "            <td>[]</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_display_html(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.020014504, -0.013545036, 0.04353385, -0.0029185074, 0.03552278, -0.034943033, 0.013927143, 0.06566971, -0.06888468, 0.026971487]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0][\"embeddings\"]))\n",
    "print(dataset[0][\"embeddings\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authors',\n",
       " 'chunk_id',\n",
       " 'doc_id',\n",
       " 'primary_category',\n",
       " 'published',\n",
       " 'source',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'title',\n",
       " 'year']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset[0][\"metadata\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has been done with data set?\n",
    "\n",
    "![chunking-dataset.png](https://i.ibb.co/9wV70Q7/chunking-dataset.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "#### Chunking Strategies\n",
    "1. Character split (with overlapping)\n",
    "2. Recursive character split\n",
    "3. Document specific splitting\n",
    "4. Semantic Chunking\n",
    "5. Agentic?\n",
    "6. More?\n",
    "\n",
    "Introduction to chunking: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data upsert to Pinecone\n",
    "\n",
    "Let insert data to the Pinecone in batches. \n",
    "\n",
    "From our data set we need 3 columns:\n",
    "1. `id` - the ID of the chunk we want to insert\n",
    "2. `embeddings` - contains a vector embedding of the chunk. It uses `text-embeddings-3-small`\n",
    "3. `metadata` - a dictinary with additional data about the chunk. \n",
    "\n",
    "The code for upserting:\n",
    "```python\n",
    "index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    " ])\n",
    "```\n",
    "We can also upsert data directly from the `pandas.Dataframe` using `index.upsert_from_dataframe`.  \n",
    "It requires 3 columns: \n",
    "1. `values` - a column that contain a embeddings for each chunk/record\n",
    "2. `id` - an ID of a record\n",
    "3. `metadata` - a key-value pairs for record metadata\n",
    "\n",
    "Upsert from dataframe: https://docs.pinecone.io/guides/data/use-public-pinecone-datasets#upsert-a-dataset-as-a-dataframe \n",
    "Avoid quotas and limits: https://docs.pinecone.io/reference/quotas-and-limits  \n",
    "For scale-up and optimizations make sure to read: : https://docs.pinecone.io/guides/operations/performance-tuning#increasing-throughput  \n",
    "Metadata filtering: Metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import GRPCIndex as Index\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 200) -> None:\n",
    "    df = ds.to_pandas()\n",
    "    df = df[[\"id\", \"embeddings\", \"metadata\"]]\n",
    "    df.rename(columns={\"embeddings\": \"values\"}, inplace=True)\n",
    "    index.upsert_from_dataframe(df, batch_size=batch_size, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773c5bb6e4104a99ba9461982d668606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sending upsert requests:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31d3e30fe124f3bbed2caa43d9f5a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting async responses:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsert_batch(dataset.select(range(500)), index, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 500}},\n",
      " 'total_vector_count': 500}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cc94f1e89a489c9048b872e28ef0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sending upsert requests:   0%|          | 0/79782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f6daf214f4092ba6e11999497de50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting async responses:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsert_batch(dataset, index, batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![done-next.png](https://i.ibb.co/DKX2cKz/done-next.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve against the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![semantic-search.png](https://i.ibb.co/2dWGRHn/semantic-search.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we inserted everythong to Pinecone, let's query it. The input is query text and the output is the top similar chunks.\n",
    "Steps: \n",
    "1. Encode the input text to generate embeddings \n",
    "2. Call Pinecone's query function to retrieve top K similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def encode(\n",
    "    text: str, openai_client: OpenAI, model: str = \"text-embedding-3-small\"\n",
    ") -> List[float]:\n",
    "    # Use the OpenAI API to encode the text\n",
    "    # Docs: https://platform.openai.com/docs/api-reference/embeddings\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02581052854657173, 0.03242715075612068, 0.019921164959669113, -0.021119002252817154, -0.036020658910274506, 0.023999514058232307, -0.05059434100985527, 0.05886511504650116, 0.024940671399235725, -0.006969555746763945]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "res = encode(\"What are key benefits of mistral 7B?\", openai)\n",
    "print(res[:10])\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import QueryResponse\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    query: str, index: Index, openai_client: OpenAI, top_k: int = 10\n",
    ") -> QueryResponse:\n",
    "    # Encode the query using the OpenAI API\n",
    "    query_embedding = encode(query, openai_client)\n",
    "\n",
    "    # Use the Pinecone index to query the encoded vector\n",
    "    # Docs: https://docs.pinecone.io/reference/api/data-plane/query\n",
    "    ret = index.query(\n",
    "        vector=query_embedding, top_k=top_k, include_metadata=True, include_values=False\n",
    "    )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "# Introduction\n"
     ]
    }
   ],
   "source": [
    "res = retrieve(\"What is Mistral 7B?\", index, openai)\n",
    "print(res.matches[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augment and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![workflow-rag-simple.png](https://i.ibb.co/p0gwY23/workflow-rag-simple.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final response\n",
    "\n",
    "Combine all pieces together:\n",
    "1. Perform a semantic search on input query\n",
    "2. Build a context (prompt) for a LLM\n",
    "3. Call LLM to generate a final response\n",
    "4. Return a final response and retrieved context\n",
    "\n",
    "The relevant context can be found in metadata, you can use:\n",
    "1. `title` - a Paper title\n",
    "2. `published` - a publish date\n",
    "3. `primart_category` \n",
    "4. `summary` - a paper summary\n",
    "5. `text` - a chunk text - this is the most useful info to build a context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def from_metadata(metadata: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "***\n",
    "    Title: {metadata['title']}\n",
    "    Authors: {metadata['authors'][:3]}\n",
    "    Published: {metadata['published']}\n",
    "    Paper summary: {metadata['summary']}\n",
    "    Text: {metadata['text']}\n",
    "    Source: {metadata['source']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def augment(query: str, query_results: QueryResponse) -> str:\n",
    "    context = \"\\n\".join(\n",
    "        [from_metadata(result.metadata) for result in query_results.matches]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "The answers should be clear, easy to understand, complete and comprehensive.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rag(query: str, index: Index, openai_client: OpenAI, top_k: int = 5) -> Tuple[str, QueryResponse]:\n",
    "    # 1. [RETRIEVE]: Reuse `semantic_search` function to get the top_k results\n",
    "    # 2. [AUGMENT]: Use `get_prompt` function to generate the prompt (context + question)\n",
    "    # 3. [GENERATE]: Use `llm_completion` function to generate the response\n",
    "    # 4. Return the response and query_results\n",
    "    query_results = retrieve(query, index, openai_client, top_k=top_k)\n",
    "    prompt = augment(query, query_results)\n",
    "    response = generate(prompt, openai_client)\n",
    "    return response, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B are:\n",
       "\n",
       "1. **Superior Performance**: Mistral 7B is engineered for superior performance, outperforming other models like Llama 2 13B and Llama 1 34B across various benchmarks, including reasoning, mathematics, and code generation.\n",
       "\n",
       "2. **Efficiency**: Mistral 7B is designed to be efficient, balancing high performance with reduced computational costs and inference latency. This efficiency makes it suitable for deployment in practical, real-world scenarios.\n",
       "\n",
       "3. **Innovative Attention Mechanisms**: Mistral 7B leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length effectively. These attention mechanisms contribute to the model's enhanced performance and efficiency.\n",
       "\n",
       "4. **Ease of Deployment and Fine-Tuning**: Mistral 7B is released under the Apache 2.0 license, accompanied by a reference implementation for easy deployment on cloud platforms. It is also crafted for ease of fine-tuning across a wide range of tasks, making it adaptable for various applications.\n",
       "\n",
       "5. **Superior Performance in Specific Tasks**: Mistral 7B excels in tasks such as reasoning, mathematics, and code generation, showcasing its versatility and effectiveness in specialized domains.\n",
       "\n",
       "6. **Compatibility and Integration**: Mistral 7B can be integrated with tools like the vLLM inference server, SkyPilot, and Hugging Face for streamlined deployment and usage, enhancing its compatibility with existing frameworks and platforms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What are key benefits of Mistral 7B?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
       "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
       "# 2 Architectural details\n",
       "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length</td></tr>\n",
       "<tr><td>chunk_id</td><td>3.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>published</td><td>20231010.0</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>Abstract\n",
       "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
       "# Introduction</td></tr>\n",
       "<tr><td>chunk_id</td><td>1.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>published</td><td>20231010.0</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The \"Skeleton-of-Thought\" (SoT) is a methodology proposed in the research paper titled \"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\" by Xue. The main objective of SoT is to reduce the end-to-end generation latency of large language models (LLMs) by changing the traditional sequential decoding approach to a parallel decoding approach. This approach is inspired by how humans think and write answers, where the idea is to guide the LLM to first generate a concise skeleton of the answer and then complete the contents of each skeleton point in parallel.\n",
       "\n",
       "### Methodology of Skeleton-of-Thought (SoT)\n",
       "1. **Skeleton Stage**: In this stage, SoT assembles a skeleton request using a skeleton prompt template with the question as a parameter. The LLM is guided to output a brief skeleton of the answer, and then B points are extracted from this skeleton response.\n",
       "   \n",
       "2. **Point-Expanding Stage**: Based on the skeleton, the LLM expands on each point in parallel. For each point, a point-expanding request is made to the LLM using a point-expanding prompt template. After completing all points, the point-expanding responses are concatenated to form the final answer.\n",
       "\n",
       "### Benefits of Skeleton-of-Thought (SoT)\n",
       "- SoT provides significant speed-ups across 12 LLMs.\n",
       "- It has the potential to enhance the answer quality on various question categories.\n",
       "- SoT is an initial step towards data-centric optimization for inference efficiency, aiming to make LLMs think more like humans for improved answer quality.\n",
       "\n",
       "In summary, the Skeleton-of-Thought methodology introduces a structured approach to answer generation by first creating a skeleton and then expanding on it in parallel, which can lead to faster response times and potentially higher-quality answers across different question categories."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What is skeleton of thoughts?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To apply Large Language Models (LLMs) to recommender engines, several approaches can be considered based on the information provided in the context:\n",
       "\n",
       "1. **LLMs as Recommendation Models**:\n",
       "   - LLMs can be directly adapted to serve as recommendation models. This approach involves training LLMs to complete the recommendation task without parameter tuning. Various prompt engineering methods can be employed to improve recommendation performance and reduce potential biases.\n",
       "   \n",
       "2. **LLM-enhanced Recommendation Models**:\n",
       "   - In this approach, the universal knowledge encoded in LLMs is leveraged to enhance traditional recommender systems. This can be achieved through:\n",
       "     - Inferring users' potential intentions from historical interaction data using LLMs and employing these inferred intentions to improve item retrieval in recommendation systems.\n",
       "     - Using LLMs as feature encoders to encode side information of items and users, such as item descriptions and user reviews, to provide more informative representations for traditional recommender systems.\n",
       "     - Adopting a distillation-like method to transfer LLM capacities, such as semantic encoding, to improve traditional recommenders by aligning hidden states of LLMs and traditional recommendation models through joint training.\n",
       "\n",
       "3. **LLMs as Recommendation Simulators**:\n",
       "   - LLMs can also be utilized as recommendation simulators to simulate potential user instructions in various scenarios like product search and personalized recommendations. This can help in improving the diversity of instructions and enhancing the recommendation process.\n",
       "\n",
       "4. **Addressing Challenges**:\n",
       "   - Despite the potential benefits of using LLMs in recommender systems, there are challenges that need to be addressed, such as:\n",
       "     - Performance issues in zero/few-shot settings compared to traditional ID-based recommenders.\n",
       "     - Semantic gap between LLMs and recommender systems, leading to high tuning costs and reduced understanding of personalized user behaviors.\n",
       "     - Inference latency and memory overhead challenges, especially in low-resourced environments.\n",
       "     - Limited capacities of existing LLMs in long context modeling, requiring improved approaches for processing large amounts of user-item interaction data.\n",
       "\n",
       "By considering these approaches and challenges, developers can effectively apply LLMs to recommender engines, enhancing recommendation performance and user experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"How to apply LLMs to recommender engines?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>LLM-enhanced Recommendation Models. In addition to instructing LLMs to directly provide recommendations, re- searchers also propose leveraging the universal knowledge encoded in LLMs to improve traditional recommender sys- tems. Existing approaches in this line can be divided into three main categories. The first category employs LLMs to infer usersâ potential intention from their historical interac- tion data. Furthermore, traditional recommendation/search models employ the inferred intentions to improve the re- trieval of relevant items [812, 813]. Additionally, several studies explore the use of LLMs as feature encoders. They employ LLMs to encode the side information of items and\n",
       "75\n",
       "users (e.g., itemâs descriptions and userâs reviews), thus de- riving more informative representations of users and items. These representations are then fed into traditional recom- mender systems as augmented input [814, 815]. As an- other alternative approach, several studies [816, 817] adopt a distillation-like way to transfer LLMâs capacities (e.g., semantic encoding) to improve traditional recommenders (i.e., small models). Specially, they align the hidden states of LLMs and traditional recommendation models via joint training. After training, since only the enhanced small model will be deployed online, it can avoid the huge over- head of LLMs in online service.</td></tr>\n",
       "<tr><td>chunk_id</td><td>481.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2303.18223</td></tr>\n",
       "<tr><td>summary</td><td>Language is essentially a complex, intricate system of human expressions\n",
       "governed by grammatical rules. It poses a significant challenge to develop\n",
       "capable AI algorithms for comprehending and grasping a language. As a major\n",
       "approach, language modeling has been widely studied for language understanding\n",
       "and generation in the past two decades, evolving from statistical language\n",
       "models to neural language models. Recently, pre-trained language models (PLMs)\n",
       "have been proposed by pre-training Transformer models over large-scale corpora,\n",
       "showing strong capabilities in solving various NLP tasks. Since researchers\n",
       "have found that model scaling can lead to performance improvement, they further\n",
       "study the scaling effect by increasing the model size to an even larger size.\n",
       "Interestingly, when the parameter scale exceeds a certain level, these enlarged\n",
       "language models not only achieve a significant performance improvement but also\n",
       "show some special abilities that are not present in small-scale language\n",
       "models. To discriminate the difference in parameter scale, the research\n",
       "community has coined the term large language models (LLM) for the PLMs of\n",
       "significant size. Recently, the research on LLMs has been largely advanced by\n",
       "both academia and industry, and a remarkable progress is the launch of ChatGPT,\n",
       "which has attracted widespread attention from society. The technical evolution\n",
       "of LLMs has been making an important impact on the entire AI community, which\n",
       "would revolutionize the way how we develop and use AI algorithms. In this\n",
       "survey, we review the recent advances of LLMs by introducing the background,\n",
       "key findings, and mainstream techniques. In particular, we focus on four major\n",
       "aspects of LLMs, namely pre-training, adaptation tuning, utilization, and\n",
       "capacity evaluation. Besides, we also summarize the available resources for\n",
       "developing LLMs and discuss the remaining issues for future directions.</td></tr>\n",
       "<tr><td>published</td><td>20230331.0</td></tr>\n",
       "<tr><td>title</td><td>A Survey of Large Language Models</td></tr>\n",
       "<tr><td>doc_id</td><td>2303.18223</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>LLMs as Recommendation Models. With specific methods or mechanisms, LLMs can be adapted to serve as recom- mendation models. Existing work along this line can be generally divided into two main categories. First, some methods prompt LLMs for completing the recommendation task in a zero-shot paradigm (i.e., without parameter tun- ing) [805, 806]. A series of prompt engineering methods like recency-focused and in-context learning are introduced to improve recommendation performance as well as alleviate the potential model biases [807, 808]. Second, another cat- egory of studies aim to specialize LLMs for personalized recommendation through instruction tuning [357, 809]. Spe- cially, high-quality instruction data is key to adapt LLMs to the recommendation tasks, which can be constructed based on user-item interactions with heuristic templates. To further improve the instruction diversity, InstructRec [357] employs self-instruct technique to simulate large amounts of potential user instructions in various scenarios like product search and personalized recommendations. In addition to representing each item by its text description, there is also growing attention on extending LLMâs vocabulary with semantic identifiers in recommender systems [810, 811], to incorporate collaborative semantics into LLMs.</td></tr>\n",
       "<tr><td>chunk_id</td><td>480.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2303.18223</td></tr>\n",
       "<tr><td>summary</td><td>Language is essentially a complex, intricate system of human expressions\n",
       "governed by grammatical rules. It poses a significant challenge to develop\n",
       "capable AI algorithms for comprehending and grasping a language. As a major\n",
       "approach, language modeling has been widely studied for language understanding\n",
       "and generation in the past two decades, evolving from statistical language\n",
       "models to neural language models. Recently, pre-trained language models (PLMs)\n",
       "have been proposed by pre-training Transformer models over large-scale corpora,\n",
       "showing strong capabilities in solving various NLP tasks. Since researchers\n",
       "have found that model scaling can lead to performance improvement, they further\n",
       "study the scaling effect by increasing the model size to an even larger size.\n",
       "Interestingly, when the parameter scale exceeds a certain level, these enlarged\n",
       "language models not only achieve a significant performance improvement but also\n",
       "show some special abilities that are not present in small-scale language\n",
       "models. To discriminate the difference in parameter scale, the research\n",
       "community has coined the term large language models (LLM) for the PLMs of\n",
       "significant size. Recently, the research on LLMs has been largely advanced by\n",
       "both academia and industry, and a remarkable progress is the launch of ChatGPT,\n",
       "which has attracted widespread attention from society. The technical evolution\n",
       "of LLMs has been making an important impact on the entire AI community, which\n",
       "would revolutionize the way how we develop and use AI algorithms. In this\n",
       "survey, we review the recent advances of LLMs by introducing the background,\n",
       "key findings, and mainstream techniques. In particular, we focus on four major\n",
       "aspects of LLMs, namely pre-training, adaptation tuning, utilization, and\n",
       "capacity evaluation. Besides, we also summarize the available resources for\n",
       "developing LLMs and discuss the remaining issues for future directions.</td></tr>\n",
       "<tr><td>published</td><td>20230331.0</td></tr>\n",
       "<tr><td>title</td><td>A Survey of Large Language Models</td></tr>\n",
       "<tr><td>doc_id</td><td>2303.18223</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about comparisons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know the answer to this question as the context provided does not contain information about the models Mistral, Kosmos, and Palm for a side-by-side comparison."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\n",
    "    \"Write side by side summaries between mistral, kosmos, palm. If there is key difference between them, what is it\",\n",
    "    index,\n",
    "    openai,\n",
    "    top_k=10,\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>Task Zero-shot One-shot Few-shot (k = 4) LLM KOSMOS-1 LLM KOSMOS-1 LLM KOSMOS-1 StoryCloze HellaSwag 72.9 50.4 72.1 50.0 72.9 50.2 72.2 50.0 73.1 50.4 72.3 50.3 Winograd Winogrande 71.6 56.7 69.8 54.8 71.2 56.7 68.4 54.5 70.9 57.0 69.8 55.7 PIQA 73.2 72.9 73.0 72.5 72.6 72.3 BoolQ CB COPA 56.4 39.3 68.0 56.4 44.6 63.0 55.1 41.1 69.0 57.2 48.2 64.0 58.7 42.9 69.0 59.2 53.6 64.0 Average 61.1 60.5 61.2 60.9 61.8 62.2\n",
       "Table 13: Performance comparisons of language tasks between KOSMOS-1 and LLM. We use the same textual data and training setup to reimplement a language model. Both models do not use instruction tuning for fair comparisons.\n",
       "# 4.9 Cross-modal Transfer</td></tr>\n",
       "<tr><td>chunk_id</td><td>53.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2302.14045</td></tr>\n",
       "<tr><td>summary</td><td>A big convergence of language, multimodal perception, action, and world\n",
       "modeling is a key step toward artificial general intelligence. In this work, we\n",
       "introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\n",
       "general modalities, learn in context (i.e., few-shot), and follow instructions\n",
       "(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\n",
       "multimodal corpora, including arbitrarily interleaved text and images,\n",
       "image-caption pairs, and text data. We evaluate various settings, including\n",
       "zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\n",
       "of tasks without any gradient updates or finetuning. Experimental results show\n",
       "that Kosmos-1 achieves impressive performance on (i) language understanding,\n",
       "generation, and even OCR-free NLP (directly fed with document images), (ii)\n",
       "perception-language tasks, including multimodal dialogue, image captioning,\n",
       "visual question answering, and (iii) vision tasks, such as image recognition\n",
       "with descriptions (specifying classification via text instructions). We also\n",
       "show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\n",
       "from language to multimodal, and from multimodal to language. In addition, we\n",
       "introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\n",
       "capability of MLLMs.</td></tr>\n",
       "<tr><td>published</td><td>20230227.0</td></tr>\n",
       "<tr><td>title</td><td>Language Is Not All You Need: Aligning Perception with Language Models</td></tr>\n",
       "<tr><td>doc_id</td><td>2302.14045</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td># 3 Results\n",
       "We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
       "â¢ Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\n",
       "â¢ World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\n",
       "â¢ Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\n",
       "â¢ Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\n",
       "â¢ Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\n",
       "â¢ Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)</td></tr>\n",
       "<tr><td>chunk_id</td><td>8.0</td></tr>\n",
       "<tr><td>year</td><td>2023.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>published</td><td>20231010.0</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>WAhk6cGnD5ROlyquY/edit?usp=sharing b. Three summaries, A, B and C: See below 2. Annotations: a. For each summary, check all the information in the information checklist in the following way, i. If the information is covered in the summary, check the corresponding checkbox ii. | Otherwise, leave it blank. 2- Fluency: There are multiple candidate summaries (3) for each system and we would like to rank the summaries from most most fluent to least. Specifically, for each summary, answer the following questions:</td></tr>\n",
       "<tr><td>chunk_id</td><td>80.0</td></tr>\n",
       "<tr><td>year</td><td>2021.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2110.08499</td></tr>\n",
       "<tr><td>summary</td><td>We introduce PRIMERA, a pre-trained model for multi-document representation\n",
       "with a focus on summarization that reduces the need for dataset-specific\n",
       "architectures and large amounts of fine-tuning labeled data. PRIMERA uses our\n",
       "newly proposed pre-training objective designed to teach the model to connect\n",
       "and aggregate information across documents. It also uses efficient\n",
       "encoder-decoder transformers to simplify the processing of concatenated input\n",
       "documents. With extensive experiments on 6 multi-document summarization\n",
       "datasets from 3 different domains on zero-shot, few-shot and full-supervised\n",
       "settings, PRIMERA outperforms current state-of-the-art dataset-specific and\n",
       "pre-trained models on most of these settings with large margins. The code and\n",
       "pre-trained models can be found at \\url{https://github.com/allenai/PRIMER}.</td></tr>\n",
       "<tr><td>published</td><td>20211016.0</td></tr>\n",
       "<tr><td>title</td><td>PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization</td></tr>\n",
       "<tr><td>doc_id</td><td>2110.08499</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1 - Query Comprehension\n",
    "\n",
    "Your task is to figure out how to execute the following query:  \n",
    "\n",
    "`Write side by side summaries between mistral, kosmos, palm. If there is key difference between them, what is it`\n",
    "\n",
    "Hints:\n",
    "1. JSON response from GPT: https://platform.openai.com/docs/api-reference/chat/create \n",
    "2. Use LLM to extract information from the query\n",
    "3. Apply the right techniques of RAG to construct the best response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a query comprehension tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2 - Paper listing and filtering\n",
    "\n",
    "Adapt RAG model to work on paper listing, ie it should be able to answer this type of question:  \n",
    "`List me papers names and sources that are published in 2023 about e-commerce search techniques`\n",
    "\n",
    "Hints:\n",
    "1. Use metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata (`metadata_filter={\"published\": {\"gt\": 20230101}}`)\n",
    "2. Use LLM (prompt engineering) to extract the useful data from the query (search term, date, task, etc)\n",
    "3. JSON reponse from ChatGPT - https://platform.openai.com/docs/api-reference/chat/create   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement paper listing and metadata filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "An example how simple RAG application could be done with frameworks.\n",
    "\n",
    "Langchain: https://www.langchain.com/langchain \n",
    "\n",
    "```python\n",
    "!pip install -qU \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index, embedding=embeddings, text_key=\"text\", pinecone_api_key=PINECONE_API_KEY, index_name=INDEX_NAME,)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "langchain_rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "langchain_rag.invoke(\"What are all benefits of using Mistral 7B?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune vs RAG vs Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag-vs-prompt-vs-finetune.png](https://i.ibb.co/C8rvnTY/Screenshot-2024-05-30-at-21-17-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
