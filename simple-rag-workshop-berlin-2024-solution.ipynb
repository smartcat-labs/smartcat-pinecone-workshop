{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title.png](https://i.ibb.co/2KmT38V/title.png)\n",
    "\n",
    "![objectives.png](https://i.ibb.co/fxbWnNQ/objectives.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!   pip install -qU \\\n",
    "    openai==1.30 \\\n",
    "    pinecone-client==4.1.0 \\\n",
    "    datasets==2.19 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def chunk_display_html(chunk: Dict[str, str]) -> str:\n",
    "    html_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table {{\n",
    "        font-family: arial, sans-serif;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    td, th {{\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Key</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Title</td>\n",
    "            <td>{title}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>DOI</td>\n",
    "            <td>{doi}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk ID</td>\n",
    "            <td>{chunk_id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk</td>\n",
    "            <td>{chunk}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ID</td>\n",
    "            <td>{id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Summary</td>\n",
    "            <td>{summary}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Source</td>\n",
    "            <td>{source}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Authors</td>\n",
    "            <td>{authors}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>{categories}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comment</td>\n",
    "            <td>{comment}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Journal Reference</td>\n",
    "            <td>{journal_ref}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Primary Category</td>\n",
    "            <td>{primary_category}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Published</td>\n",
    "            <td>{published}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Updated</td>\n",
    "            <td>{updated}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>References</td>\n",
    "            <td>{references}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # Format the HTML with the generated rows\n",
    "    html_output = html_template.format(\n",
    "        doi=chunk.get(\"doi\", \"N/A\"),\n",
    "        chunk_id=chunk.get(\"chunk-id\", \"N/A\"),\n",
    "        chunk=chunk.get(\"chunk\", \"N/A\"),\n",
    "        id=chunk.get(\"id\", \"N/A\"),\n",
    "        title=chunk.get(\"title\", \"N/A\"),\n",
    "        summary=chunk.get(\"summary\", \"N/A\"),\n",
    "        source=chunk.get(\"source\", \"N/A\"),\n",
    "        authors=chunk.get(\"authors\", \"N/A\"),\n",
    "        categories=chunk.get(\"categories\", \"N/A\"),\n",
    "        comment=chunk.get(\"comment\", \"N/A\"),\n",
    "        journal_ref=chunk.get(\"journal_ref\", \"N/A\"),\n",
    "        primary_category=chunk.get(\"primary_category\", \"N/A\"),\n",
    "        published=chunk.get(\"published\", \"N/A\"),\n",
    "        updated=chunk.get(\"updated\", \"N/A\"),\n",
    "        references=chunk.get(\"references\", \"N/A\"),\n",
    "    )\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_retrieved_context(context_response):\n",
    "    # HTML template for the main container and individual tables\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            margin: 10px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #dddddd;\n",
    "        }}\n",
    "        table {{\n",
    "            font-family: arial, sans-serif;\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        td, th {{\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            {tables}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate HTML table for a single dictionary\n",
    "    def generate_table_for_dict(data):\n",
    "        rows = \"\\n\".join(\n",
    "            \"<tr><td>{key}</td><td>{value}</td></tr>\".format(\n",
    "                key=key, value=value if value is not None else \"N/A\"\n",
    "            )\n",
    "            for key, value in data.items()\n",
    "        )\n",
    "        table_html = \"\"\"\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Key</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                {rows}\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\".format(\n",
    "            rows=rows\n",
    "        )\n",
    "        return table_html\n",
    "\n",
    "    # Generate HTML tables for all dictionaries in the list\n",
    "    tables = \"\\n\".join(\n",
    "        generate_table_for_dict(data[\"metadata\"]) for data in context_response\n",
    "    )\n",
    "\n",
    "    # Format the main HTML with the generated tables\n",
    "    html_output = html_template.format(tables=tables)\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_markdown(content: str) -> None:\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-away-rag.png](https://i.ibb.co/Y288TWR/step-away-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI\n",
    "\n",
    "Enter the OpenAI API key and instantiate the OpenAI clinet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement request to OpenAI GPT Models\n",
    "\n",
    "Implement a function that will send a prompt to an LLM and return an answer.\n",
    "The OpenAI client has the following signature:\n",
    "`openai_client.chat.completions.create(model: str, messages=List[Dict[str, str]])`\n",
    "\n",
    "API reference: https://platform.openai.com/docs/api-reference/chat/create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_completion(prompt: str, openai_client: OpenAI, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output is markdown\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of Germany is Berlin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"What is the capital of Germany?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 25th person to land on the moon was Harrison Schmitt. He was part of the Apollo 17 mission in December 1972."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"Who is 25th person that landed on the moon?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hallucinations-problem.png](https://i.ibb.co/gMvNZC6/hallucinations-problem.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Key Benefits of Mistral 7B:\n",
       "1. **High Performance**: The Mistral 7B offers excellent performance capabilities, making it suitable for a wide range of applications.\n",
       "  \n",
       "2. **Reliability**: This model is known for its reliability and durability, ensuring that it can withstand various environmental conditions and heavy usage.\n",
       "\n",
       "3. **Energy Efficiency**: The Mistral 7B is designed to be energy-efficient, helping to reduce operational costs and environmental impact.\n",
       "\n",
       "4. **User-Friendly**: It is user-friendly and easy to operate, making it suitable for both experienced and novice users.\n",
       "\n",
       "5. **Versatility**: The Mistral 7B is versatile and can be used for different purposes, making it a flexible choice for various projects.\n",
       "\n",
       "6. **Compact Design**: Its compact design makes it easy to transport and store, ideal for users who require mobility and space-saving solutions.\n",
       "\n",
       "7. **Advanced Features**: The Mistral 7B comes equipped with advanced features that enhance its performance and usability, providing users with a seamless experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"What are key benefits of mistral 7B?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge-cutoff.png](https://i.ibb.co/ccccpxZ/knowledge-cutoff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B include:\n",
       "\n",
       "1. Superior performance and efficiency compared to other models.\n",
       "2. Outperforming the best open 13B model (Llama 2) across all evaluated benchmarks.\n",
       "3. Outperforming the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.\n",
       "4. Leveraging grouped-query attention (GQA) for faster inference.\n",
       "5. Using sliding window attention (SWA) to effectively handle sequences of arbitrary length with reduced inference cost.\n",
       "6. Providing a fine-tuned model, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_example = \"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "\n",
    "Context:\n",
    "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "\n",
    "Question: What are key benefits of Mistral 7B?\n",
    "\"\"\"\n",
    "display_markdown(llm_completion(context_example, openai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![why-rag.png](https://i.ibb.co/KF3xr64/why-rag.png)\n",
    "\n",
    "![rag-vs-prompt-vs-finetune.png](https://i.ibb.co/C8rvnTY/Screenshot-2024-05-30-at-21-17-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build a RAG?\n",
    "\n",
    "Steps:\n",
    "1. Build a knowledge base\n",
    "2. Build retrieval\n",
    "3. Augment and generate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base\n",
    "\n",
    "![build-knowledge-base.png](https://i.ibb.co/dGnjrCk/build-knowledge-base.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone \n",
    "\n",
    "Enter the Pinecone API key inside the prompt and create a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_REGION = \"eu-west-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "INDEX_NAME = \"pinecone-workshop-1\"\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'pinecone-workshop-1-2kw20wn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-workshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'pinecone-worshop-1-2kw20wn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-worshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index\n",
    "\n",
    "Create a Serverless Index with OpenAI embeddings size with cosine similarity metrics. \n",
    "The index creation requires index name, dimension of embeddings, similarity metric and serverless spec for a serverless setup.\n",
    "\n",
    "More info o serverless: https://docs.pinecone.io/reference/architecture/serverless-architecture#overview  \n",
    "Ref API: https://docs.pinecone.io/reference/api/control-plane/create_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# TODO: Create a new index with the specified name, dimension, metric, and spec\n",
    "# 1. check if the index already exists and delete it\n",
    "# 2. create a Pinecone index\n",
    "# 3. create a new Index reference object with the specified name and pool_threads\n",
    "\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "pinecone.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=VECTOR_DIMENSIONS,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
    ")\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME, pool_threads=20)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use a sample of 1000 AI papers that can be found here: https://huggingface.co/datasets/smartcat/ai-arxiv2-chunks-embedded \n",
    "\n",
    "The data set is already chunked and encoded using `text-embeddings-3-small` so we can just load and upsert it to the Pinecone.\n",
    "\n",
    "If you want to play with chunking strategies and embeddings, you can find the full data set here: https://huggingface.co/datasets/jamescalam/ai-arxiv2\n",
    "\n",
    "Dataset API reference: https://huggingface.co/docs/datasets/en/index  \n",
    "Slicing and indexing: https://huggingface.co/docs/datasets/en/access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'embeddings', 'metadata'],\n",
       "    num_rows: 79782\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"smartcat/ai-arxiv2-chunks-embedded\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "    table {\n",
       "        font-family: arial, sans-serif;\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    td, th {\n",
       "        border: 1px solid #dddddd;\n",
       "        text-align: left;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Key</th>\n",
       "            <th>Value</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Title</td>\n",
       "            <td>Foundations of Vector Retrieval</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DOI</td>\n",
       "            <td>2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk ID</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk</td>\n",
       "            <td>4 2 0 2\n",
       "n a J 7 1 ] S D . s c [\n",
       "1 v 0 5 3 9 0 . 1 0 4 2 : v i X r a\n",
       "Sebastian Bruch\n",
       "# Foundations of Vector Retrieval\n",
       "# Preface\n",
       "We are witness to a few years of remarkable developments in Artificial Intelligence with the use of advanced machine learning algorithms, and in particular, deep learning. Gargantuan, complex neural networks that can learn through self-supervisionâand quickly so with the aid of special- ized hardwareâtransformed the research landscape so dramatically that, incremental overnight it seems, many fields experienced not the usual, progress, but rather a leap forward. Machine translation, natural language understanding, information retrieval, recommender systems, and computer vision are but a few examples of research areas that have had to grapple with the shock. Countless other disciplines beyond computer science such as robotics, biology, and chemistry too have benefited from deep learning.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ID</td>\n",
       "            <td>2401.09350#0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Summary</td>\n",
       "            <td>Vectors are universal mathematical objects that can represent text, images,\n",
       "speech, or a mix of these data modalities. That happens regardless of whether\n",
       "data is represented by hand-crafted features or learnt embeddings. Collect a\n",
       "large enough quantity of such vectors and the question of retrieval becomes\n",
       "urgently relevant: Finding vectors that are more similar to a query vector.\n",
       "This monograph is concerned with the question above and covers fundamental\n",
       "concepts along with advanced data structures and algorithms for vector\n",
       "retrieval. In doing so, it recaps this fascinating topic and lowers barriers of\n",
       "entry into this rich area of research.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Source</td>\n",
       "            <td>http://arxiv.org/pdf/2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Authors</td>\n",
       "            <td>Sebastian Bruch</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Categories</td>\n",
       "            <td>cs.DS, cs.IR</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Comment</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Journal Reference</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Primary Category</td>\n",
       "            <td>cs.DS</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Published</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Updated</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>References</td>\n",
       "            <td>[]</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_display_html(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.020014504, -0.013545036, 0.04353385, -0.0029185074, 0.03552278, -0.034943033, 0.013927143, 0.06566971, -0.06888468, 0.026971487]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0][\"embeddings\"]))\n",
    "print(dataset[0][\"embeddings\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authors',\n",
       " 'chunk_id',\n",
       " 'doc_id',\n",
       " 'primary_category',\n",
       " 'published',\n",
       " 'source',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'title']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset[0][\"metadata\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data upsert to Pinecone\n",
    "\n",
    "Let insert data to the Pinecone in batches. \n",
    "\n",
    "From our data set we need 3 columns:\n",
    "1. `id` - the ID of the chunk we want to insert\n",
    "2. `embeddings` - contains a vector embedding of the chunk. It uses `text-embeddings-3-small`\n",
    "3. `metadata` - a dictinary with additional data about the chunk. \n",
    "\n",
    "Metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata \n",
    "\n",
    "### Note on optimization\n",
    "\n",
    "We are going to add `async_req=True` parameter. Upsert will return futures that we need to wait.\n",
    "\n",
    "Optimization tips:\n",
    "1. Deploy application at the same region\n",
    "2. Use batching upsert\n",
    "3. Use parallelized upsert\n",
    "4. Use GRPCIndex, but make sure to add backoff for throttling\n",
    "5. Use namespaces and metadata filtering\n",
    "6. Avoid quotas and limits: https://docs.pinecone.io/reference/quotas-and-limits\n",
    "\n",
    "For scale-up and optimizations make sure to read: : https://docs.pinecone.io/guides/operations/performance-tuning#increasing-throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pinecone import Index\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 100) -> None:\n",
    "    # TODO: Upsert the vectors to the Pinecone index\n",
    "    # 1. Iterate over the dataset in batches\n",
    "    # 2. Select the batch\n",
    "    # 3. Extract the IDs, embeddings, and metadata\n",
    "    # 4. Upsert the vectors to the Pinecone index\n",
    "    # 5. Optimization: Use req_async=True to make the upserts asynchronous and wait for futures to complete\n",
    "\n",
    "    futures = []\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        i_end = min(i + batch_size, len(ds))\n",
    "        batch = ds.select(range(i, i_end))\n",
    "\n",
    "        ids = batch[\"id\"]\n",
    "        embeddings = batch[\"embeddings\"]\n",
    "        metadata = batch[\"metadata\"]\n",
    "        futures.append(\n",
    "            index.upsert(vectors=list(zip(ids, embeddings, metadata)), async_req=True)\n",
    "        )\n",
    "\n",
    "    for future in tqdm(futures, total=len(futures), desc=\"Upsert to Pinecone\"):\n",
    "        future.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upsert to Pinecone: 100%|██████████| 25/25 [00:00<00:00, 131.28it/s]\n"
     ]
    }
   ],
   "source": [
    "upsert_batch(dataset.select(range(500)), index, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 500}},\n",
      " 'total_vector_count': 500}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upsert to Pinecone: 100%|██████████| 798/798 [00:00<00:00, 3071.12it/s]\n"
     ]
    }
   ],
   "source": [
    "upsert_batch(dataset, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 82082}},\n",
      " 'total_vector_count': 82082}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has been done with data set?\n",
    "\n",
    "![chunking-dataset.png](https://i.ibb.co/9wV70Q7/chunking-dataset.png)\n",
    "\n",
    "#### Chunking Strategies\n",
    "1. Character split (with overlapping)\n",
    "2. Recursive character split\n",
    "3. Document specific splitting\n",
    "4. Semantic Chunking\n",
    "5. Agentic?\n",
    "6. More?\n",
    "\n",
    "Introduction to chunking: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "### Revisit Agenda\n",
    "\n",
    "![done-next.png](https://i.ibb.co/QYC9nrb/done-next.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve against the query\n",
    "\n",
    "Once we inserted everythong to Pinecone, let's query it. The input is query text and the output is the top similar chunks.\n",
    "Steps: \n",
    "1. Encode the input text to generate embeddings\n",
    "2. Call Pinecone's query function to retrieve top K similar results\n",
    "\n",
    "\n",
    "Embedding API Ref: https://platform.openai.com/docs/api-reference/embeddings  \n",
    "Query API Ref: https://docs.pinecone.io/reference/api/data-plane/query \n",
    "\n",
    "![semantic-search.png](https://i.ibb.co/2dWGRHn/semantic-search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def encode(\n",
    "    text: str, openai_client: OpenAI, model: str = \"text-embedding-3-small\"\n",
    ") -> List[float]:\n",
    "    # TODO: create embeddings using OpenAI API\n",
    "    # 1. Call openai_client.embeddings.create with the model and text\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02581052854657173, 0.03242715075612068, 0.019921164959669113, -0.021119002252817154, -0.036020658910274506, 0.023999514058232307, -0.05059434100985527, 0.05886511504650116, 0.024940671399235725, -0.006969555746763945]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "res = encode(\"What are key benefits of mistral 7B?\", openai)\n",
    "print(res[:10])\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import QueryResponse\n",
    "\n",
    "\n",
    "def semantic_search(\n",
    "    query: str, index: Index, openai_client: OpenAI, top_k: int = 10\n",
    ") -> QueryResponse:\n",
    "    # TODO: Perform semantic search on input query\n",
    "    # 1. encode the query using OpenAI embeddings API\n",
    "    # 2. call index.query with the query embedding to get the top_k results\n",
    "    # 3. Include metadata and exclude values in the query\n",
    "    query_embedding = encode(query, openai_client)\n",
    "    ret = index.query(\n",
    "        vector=query_embedding, top_k=top_k, include_metadata=True, include_values=False\n",
    "    )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "# Introduction\n"
     ]
    }
   ],
   "source": [
    "res = semantic_search(\"What is Mistral 7B?\", index, openai)\n",
    "print(res.matches[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augment and generate\n",
    "\n",
    "![workflow-rag-simple.png](https://i.ibb.co/p0gwY23/workflow-rag-simple.png)\n",
    "\n",
    "### Generate a final response\n",
    "\n",
    "Combine all pieces together:\n",
    "1. Perform a semantic search on input query\n",
    "2. Build a context (prompt) for a LLM\n",
    "3. Call LLM to generate a final response\n",
    "4. Return a final response and retrieved context\n",
    "\n",
    "The relevant context can be found in metadata, you can use:\n",
    "1. `title` - a Paper title\n",
    "2. `published` - a publish date\n",
    "3. `primart_category` \n",
    "4. `summary` - a paper summary\n",
    "5. `text` - a chunk text - this is the most useful info to build a context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def from_metadata(metadata: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "***************************************\n",
    "Title: {metadata['title']}\n",
    "Authors: {metadata['authors'][:5]}\n",
    "Published: {metadata['published']}\n",
    "Primary category: {metadata['primary_category']}\n",
    "Paper summary: {metadata['summary']}\n",
    "Text: {metadata['text']}\n",
    "***************************************\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(query: str, query_results: QueryResponse) -> str:\n",
    "    context = \"\\n\".join(\n",
    "        [from_metadata(result.metadata) for result in query_results.matches]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "You are a helpful assistant that answers questions. Answer the following question: \"{query}\"\n",
    "-----------------------------------\n",
    "Use the following context to answer the question {context}\n",
    "-----------------------------------\n",
    "If the answer is not in the context, answer with there is not enough information to answer the question.\n",
    "The answers should be clear, easy to understand, complete and comprehensive.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rag(query: str, index: Index, openai_client: OpenAI, top_k: int = 5) -> Tuple[str, QueryResponse]:\n",
    "    # TODO: Wire up the RAG pipeline\n",
    "    # 1. Perform semantic search to retrieve the context (QueryResponse)\n",
    "    # 2. Generate the prompt using the context (QueryResponse). Make sure to include the query and relevant metadata in the prompt\n",
    "    # 3. Call the LLM completion with the prompt to get the response\n",
    "    query_results = semantic_search(query, index, openai_client, top_k=top_k)\n",
    "    prompt = get_prompt(query, query_results)\n",
    "    response = llm_completion(prompt, openai_client)\n",
    "    return response, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Key Benefits of Mistral 7B:\n",
       "\n",
       "1. **Superior Performance**: Mistral 7B is engineered for superior performance, outperforming other models across various benchmarks, especially in reasoning, mathematics, and code generation.\n",
       "\n",
       "2. **Efficiency**: Mistral 7B is designed to be efficient, striking a balance between high performance and computational costs. It leverages innovative attention mechanisms like grouped-query attention (GQA) and sliding window attention (SWA) to enhance efficiency.\n",
       "\n",
       "3. **Ease of Deployment**: Mistral 7B is released under the Apache 2.0 license, accompanied by a reference implementation that facilitates easy deployment on various platforms such as AWS, GCP, or Azure. Integration with tools like vLLM and Hugging Face is streamlined for easier deployment.\n",
       "\n",
       "4. **Adaptability**: Mistral 7B is crafted for ease of fine-tuning across a wide range of tasks, showcasing its adaptability and versatility in real-world applications.\n",
       "\n",
       "5. **Model Fine-Tuning**: Mistral 7B offers the capability for fine-tuning models to follow specific instructions, surpassing other models in both human and automated benchmarks.\n",
       "\n",
       "6. **Balanced Model**: Mistral 7B demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference, addressing the challenges of escalating model sizes and computational costs in the NLP domain.\n",
       "\n",
       "7. **Innovative Attention Mechanisms**: The model leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length effectively, contributing to its enhanced performance and efficiency.\n",
       "\n",
       "### Note:\n",
       "If you need more specific details or have any other questions, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What are key benefits of Mistral 7B?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>3.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
       "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
       "# 2 Architectural details\n",
       "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>1.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Abstract\n",
       "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
       "# Introduction</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ChatGPT doesn't have a personality of its own, as it is an AI language model designed to assist with a wide range of tasks and provide helpful responses based on the input it receives. Its responses are generated based on patterns in the data it has been trained on and the context of the conversation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"Does ChatGPT have personality?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "Based on the paper titled \"The Self-Perception and Political Biases of ChatGPT\" published in 20230414, ChatGPT does exhibit personality traits. The paper reveals that ChatGPT perceives itself as highly open and agreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of test-takers with the least pronounced dark traits.\n",
       "\n",
       "### Paper Title and Year of Publish:\n",
       "- **Title:** The Self-Perception and Political Biases of ChatGPT\n",
       "- **Year of Publish:** 20230414"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = rag(\"Does ChatGPT have personality? Write also paper title and year of publish\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Answer: \n",
       "\n",
       "**Retrieval augmented generation** refers to a paradigm where a generative model is enhanced by incorporating a retrieval mechanism to improve the quality of generated outputs. This approach allows the model to retrieve relevant information from external sources, such as a knowledge base or a set of documents, to augment the generation process. By combining the capabilities of generative models with the retrieval of external knowledge, retrieval augmented generation aims to enhance the accuracy and relevance of the generated content.\n",
       "\n",
       "In the provided context, retrieval augmented generation is discussed in the context of language models that utilize retrieval mechanisms to enhance their performance in tasks such as language modeling, question answering, and knowledge-intensive natural language processing tasks. The integration of retrieval mechanisms allows these models to access external knowledge sources, leading to improved performance in generating text and answering queries.\n",
       "\n",
       "If you need more specific details or examples related to retrieval augmented generation in the context of language models, please let me know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = rag(\n",
    "    \"What is retrieval augmented generation?\",\n",
    "    index,\n",
    "    openai,\n",
    "    top_k=20\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Difference between Mistral and Kosmos\n",
       "\n",
       "Based on the provided context, Mistral and Kosmos are both large language models engineered for superior performance and efficiency. However, they differ in their specific capabilities and focus areas:\n",
       "\n",
       "- **Mistral**:\n",
       "  - **Publication Date**: 20231010\n",
       "  - **Primary Category**: cs.CL\n",
       "  - **Key Features**:\n",
       "    - Engineered for superior performance and efficiency\n",
       "    - Outperforms Llama 2 13B across all evaluated benchmarks\n",
       "    - Leverages grouped-query attention (GQA) and sliding window attention (SWA) for faster inference\n",
       "    - Provides a model fine-tuned to follow instructions, surpassing the Llama 2 13B -- Chat model\n",
       "    - Released under the Apache 2.0 license\n",
       "\n",
       "- **Kosmos**:\n",
       "  - **Publication Date**: 20230227\n",
       "  - **Primary Category**: cs.CL\n",
       "  - **Key Features**:\n",
       "    - A Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions\n",
       "    - Trained on web-scale multimodal corpora, including text and images\n",
       "    - Achieves impressive performance in language understanding, generation, OCR-free NLP, perception-language tasks, and vision tasks\n",
       "    - Benefits from cross-modal transfer, transferring knowledge between language and multimodal domains\n",
       "    - Introduces a dataset of Raven IQ test for diagnosing nonverbal reasoning capability\n",
       "\n",
       "In summary, Mistral is focused on language model efficiency and performance, while Kosmos is a multimodal model that excels in various tasks related to language understanding, generation, and perception."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = rag(\"The difference between Mistral and Kosmos?\", index, openai, top_k=20)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "Now, we've got all concepts right, we can explore framework Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index, embedding=embeddings, text_key=\"text\", pinecone_api_key=PINECONE_API_KEY, index_name=INDEX_NAME,)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "langchain_rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are all benefits of using Mistral 7B?',\n",
       " 'answer': 'The benefits of using Mistral 7B include superior performance and efficiency, easy deployment on cloud platforms, integration with Hugging Face, and ease of fine-tuning across various tasks.\\n',\n",
       " 'sources': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_rag.invoke(\"What are all benefits of using Mistral 7B?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG - TBA\n",
    "\n",
    "TODO: Add examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key difference between Mistral 7B and Palm is that Mistral 7B is a 7-billion-parameter language model engineered for superior performance and efficiency. It outperforms Llama 2 13B across all evaluated benchmarks and Llama 1 34B in reasoning, mathematics, and code generation. On the other hand, the context does not provide specific information about Palm to make a direct comparison."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\n",
    "    \"The key difference between Mistral 7B and Palm?\", index, openai, top_k=20\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Time\n",
    "\n",
    "### Query Time\n",
    "\n",
    "### Evaluations\n",
    "\n",
    "### Agentic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Better embeddings\n",
    "# Better chunking\n",
    "# Better indexing\n",
    "# Better query understanding\n",
    "# History management\n",
    "# Evaluation\n",
    "# Ranking (Context Limitation)\n",
    "# Guardrails\n",
    "# Frameworks (langchain / canopy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
