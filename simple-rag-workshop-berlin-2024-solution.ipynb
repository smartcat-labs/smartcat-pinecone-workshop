{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title.png](https://i.ibb.co/2KmT38V/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![objectives.png](https://i.ibb.co/fxbWnNQ/objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.1.1 requires pinecone-client<4.0.0,>=3.2.2, but you have pinecone-client 4.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!   pip install -qU \\\n",
    "    openai==1.30 \\\n",
    "    pinecone-client==4.1.0 \\\n",
    "    datasets==2.19 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def chunk_display_html(chunk: Dict[str, str]) -> str:\n",
    "    html_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table {{\n",
    "        font-family: arial, sans-serif;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    td, th {{\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Key</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Title</td>\n",
    "            <td>{title}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>doi</td>\n",
    "            <td>{doi}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk ID</td>\n",
    "            <td>{chunk_id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>chunk</td>\n",
    "            <td>{chunk}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>id</td>\n",
    "            <td>{id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Summary</td>\n",
    "            <td>{summary}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Source</td>\n",
    "            <td>{source}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Authors</td>\n",
    "            <td>{authors}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>{categories}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comment</td>\n",
    "            <td>{comment}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Journal Reference</td>\n",
    "            <td>{journal_ref}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Primary Category</td>\n",
    "            <td>{primary_category}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Published</td>\n",
    "            <td>{published}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Updated</td>\n",
    "            <td>{updated}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>References</td>\n",
    "            <td>{references}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # Format the HTML with the generated rows\n",
    "    html_output = html_template.format(\n",
    "        doi=chunk.get(\"doi\", \"N/A\"),\n",
    "        chunk_id=chunk.get(\"chunk-id\", \"N/A\"),\n",
    "        chunk=chunk.get(\"chunk\", \"N/A\"),\n",
    "        id=chunk.get(\"id\", \"N/A\"),\n",
    "        title=chunk.get(\"title\", \"N/A\"),\n",
    "        summary=chunk.get(\"summary\", \"N/A\"),\n",
    "        source=chunk.get(\"source\", \"N/A\"),\n",
    "        authors=chunk.get(\"authors\", \"N/A\"),\n",
    "        categories=chunk.get(\"categories\", \"N/A\"),\n",
    "        comment=chunk.get(\"comment\", \"N/A\"),\n",
    "        journal_ref=chunk.get(\"journal_ref\", \"N/A\"),\n",
    "        primary_category=chunk.get(\"primary_category\", \"N/A\"),\n",
    "        published=chunk.get(\"published\", \"N/A\"),\n",
    "        updated=chunk.get(\"updated\", \"N/A\"),\n",
    "        references=chunk.get(\"references\", \"N/A\"),\n",
    "    )\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_retrieved_context(context_response):\n",
    "    # HTML template for the main container and individual tables\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            margin: 10px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #dddddd;\n",
    "        }}\n",
    "        table {{\n",
    "            font-family: arial, sans-serif;\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        td, th {{\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            {tables}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate HTML table for a single dictionary\n",
    "    def generate_table_for_dict(data):\n",
    "        rows = \"\\n\".join(\n",
    "            \"<tr><td>{key}</td><td>{value}</td></tr>\".format(\n",
    "                key=key, value=value if value is not None else \"N/A\"\n",
    "            )\n",
    "            for key, value in data.items()\n",
    "        )\n",
    "        table_html = \"\"\"\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Key</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                {rows}\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\".format(\n",
    "            rows=rows\n",
    "        )\n",
    "        return table_html\n",
    "\n",
    "    # Generate HTML tables for all dictionaries in the list\n",
    "    tables = \"\\n\".join(\n",
    "        generate_table_for_dict(data[\"metadata\"]) for data in context_response\n",
    "    )\n",
    "\n",
    "    # Format the main HTML with the generated tables\n",
    "    html_output = html_template.format(tables=tables)\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_markdown(content: str) -> None:\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-away-rag.png](https://i.ibb.co/Y288TWR/step-away-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the OpenAI API key and instantiate the OpenAI clinet.\n",
    "\n",
    "Copy this openai API key `sk-XXXXXXXXXX` into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement request to OpenAI GPT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement a function that will send a prompt to an LLM and return an answer.  \n",
    "The OpenAI client has the following signature:  \n",
    "```python \n",
    "openai_client.chat.completions.create(model: str, messages=List[Dict[str, str]])\n",
    "```\n",
    "\n",
    "API reference: https://platform.openai.com/docs/api-reference/chat/create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_completion(prompt: str, openai_client: OpenAI, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output is markdown\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of Germany is Berlin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"What is the capital of Germany?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 25th person to land on the moon was astronaut Harrison Schmitt. He was a member of the Apollo 17 mission in December 1972 and was the only geologist to walk on the moon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"Who is 25th person that landed on the moon?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hallucinations-problem.png](https://i.ibb.co/gMvNZC6/hallucinations-problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Mistral 7B is a powerful and versatile wind turbine that offers several key benefits:\n",
       "\n",
       "1. **High Efficiency**: The Mistral 7B is designed to maximize energy production with its high-efficiency blades and generator, ensuring that it can generate a significant amount of electricity from the wind.\n",
       "\n",
       "2. **Robust Construction**: This wind turbine is built to withstand harsh weather conditions and has a durable construction that ensures long-term reliability and performance.\n",
       "\n",
       "3. **Quiet Operation**: The Mistral 7B is designed to operate quietly, making it suitable for residential areas or locations where noise levels need to be minimized.\n",
       "\n",
       "4. **Easy Installation**: With its user-friendly design and easy installation process, the Mistral 7B can be set up quickly and efficiently, saving time and effort.\n",
       "\n",
       "5. **Remote Monitoring**: The Mistral 7B can be equipped with remote monitoring capabilities, allowing users to track its performance and troubleshoot any issues from a distance.\n",
       "\n",
       "Overall, the Mistral 7B offers a combination of efficiency, durability, and ease of use, making it a reliable choice for renewable energy generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(llm_completion(\"What are key benefits of mistral 7B?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge-cutoff.png](https://i.ibb.co/ccccpxZ/knowledge-cutoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B include:\n",
       "\n",
       "1. Superior performance and efficiency compared to other models.\n",
       "2. Outperforming the best open 13B model (Llama 2) across all evaluated benchmarks.\n",
       "3. Outperforming the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.\n",
       "4. Leveraging grouped-query attention (GQA) for faster inference.\n",
       "5. Using sliding window attention (SWA) to effectively handle sequences of arbitrary length with reduced inference cost.\n",
       "6. Providing a fine-tuned model, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_example = \"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "\n",
    "Context:\n",
    "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "\n",
    "Question: What are key benefits of Mistral 7B?\n",
    "\"\"\"\n",
    "display_markdown(llm_completion(context_example, openai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![why-rag.png](https://i.ibb.co/KF3xr64/why-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how-to-build-rag.png](https://i.ibb.co/wgXqfFv/how-to-build-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![build-knowledge-base.png](https://i.ibb.co/dGnjrCk/build-knowledge-base.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone \n",
    "\n",
    "Enter the Pinecone API key inside the prompt and create a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_REGION = \"eu-west-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "INDEX_NAME = \"pinecone-workshop-1\"\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'pinecone-workshop-1-2kw20wn.svc.apu-57e2-42f6.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-workshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'eu-west-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'pinecone-worshop-1-2kw20wn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-worshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index\n",
    "\n",
    "Create a Serverless Index with OpenAI embeddings size with cosine similarity metrics. \n",
    "The index creation requires index name, dimension of embeddings, similarity metric and serverless spec for a serverless setup.  \n",
    "\n",
    "Pseudo code:  \n",
    "```python\n",
    "pinecone.create_index(\n",
    "    name=<add-index-name>,\n",
    "    dimension=<add-vector-dimension>,\n",
    "    metric=<add-similarity-metric>,\n",
    "    spec=ServerlessSpec(cloud=<add-cloud-name>, region=<add-region-name>)\n",
    ")\n",
    "```\n",
    "\n",
    "More info o serverless: https://docs.pinecone.io/reference/architecture/serverless-architecture#overview  \n",
    "Ref API: https://docs.pinecone.io/reference/api/control-plane/create_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Check if the index already exists and delete it\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# TODO: Create a new index with the specified name, dimension, metric, and spec\n",
    "# Use `pinecone.create_index(name=, dimension=, metric=, spec=ServerlessSpec(cloud=, region=)`\n",
    "pinecone.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=VECTOR_DIMENSIONS,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
    ")\n",
    "\n",
    "# Create a new Index reference object with the specified name and pool_threads\n",
    "index = pinecone.Index(INDEX_NAME, pool_threads=20)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use a sample of 1000 AI papers that can be found here: https://huggingface.co/datasets/smartcat/ai-arxiv2-chunks-embedded \n",
    "\n",
    "The data set is already chunked and encoded using `text-embeddings-3-small` so we can just load and upsert it to the Pinecone.\n",
    "\n",
    "If you want to play with chunking strategies and embeddings, you can find the full data set here: https://huggingface.co/datasets/jamescalam/ai-arxiv2\n",
    "\n",
    "Dataset API reference: https://huggingface.co/docs/datasets/en/index  \n",
    "Slicing and indexing: https://huggingface.co/docs/datasets/en/access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'embeddings', 'metadata'],\n",
       "    num_rows: 79782\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"smartcat/ai-arxiv2-chunks-embedded\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "    table {\n",
       "        font-family: arial, sans-serif;\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    td, th {\n",
       "        border: 1px solid #dddddd;\n",
       "        text-align: left;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Key</th>\n",
       "            <th>Value</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Title</td>\n",
       "            <td>Foundations of Vector Retrieval</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DOI</td>\n",
       "            <td>2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk ID</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk</td>\n",
       "            <td>4 2 0 2\n",
       "n a J 7 1 ] S D . s c [\n",
       "1 v 0 5 3 9 0 . 1 0 4 2 : v i X r a\n",
       "Sebastian Bruch\n",
       "# Foundations of Vector Retrieval\n",
       "# Preface\n",
       "We are witness to a few years of remarkable developments in Artificial Intelligence with the use of advanced machine learning algorithms, and in particular, deep learning. Gargantuan, complex neural networks that can learn through self-supervisionâand quickly so with the aid of special- ized hardwareâtransformed the research landscape so dramatically that, incremental overnight it seems, many fields experienced not the usual, progress, but rather a leap forward. Machine translation, natural language understanding, information retrieval, recommender systems, and computer vision are but a few examples of research areas that have had to grapple with the shock. Countless other disciplines beyond computer science such as robotics, biology, and chemistry too have benefited from deep learning.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ID</td>\n",
       "            <td>2401.09350#0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Summary</td>\n",
       "            <td>Vectors are universal mathematical objects that can represent text, images,\n",
       "speech, or a mix of these data modalities. That happens regardless of whether\n",
       "data is represented by hand-crafted features or learnt embeddings. Collect a\n",
       "large enough quantity of such vectors and the question of retrieval becomes\n",
       "urgently relevant: Finding vectors that are more similar to a query vector.\n",
       "This monograph is concerned with the question above and covers fundamental\n",
       "concepts along with advanced data structures and algorithms for vector\n",
       "retrieval. In doing so, it recaps this fascinating topic and lowers barriers of\n",
       "entry into this rich area of research.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Source</td>\n",
       "            <td>http://arxiv.org/pdf/2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Authors</td>\n",
       "            <td>Sebastian Bruch</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Categories</td>\n",
       "            <td>cs.DS, cs.IR</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Comment</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Journal Reference</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Primary Category</td>\n",
       "            <td>cs.DS</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Published</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Updated</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>References</td>\n",
       "            <td>[]</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_display_html(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.020014504, -0.013545036, 0.04353385, -0.0029185074, 0.03552278, -0.034943033, 0.013927143, 0.06566971, -0.06888468, 0.026971487]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0][\"embeddings\"]))\n",
    "print(dataset[0][\"embeddings\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authors',\n",
       " 'chunk_id',\n",
       " 'doc_id',\n",
       " 'primary_category',\n",
       " 'published',\n",
       " 'source',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'title']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset[0][\"metadata\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data upsert to Pinecone\n",
    "\n",
    "Let insert data to the Pinecone in batches. \n",
    "\n",
    "From our data set we need 3 columns:\n",
    "1. `id` - the ID of the chunk we want to insert\n",
    "2. `embeddings` - contains a vector embedding of the chunk. It uses `text-embeddings-3-small`\n",
    "3. `metadata` - a dictinary with additional data about the chunk. \n",
    "\n",
    "The code for upserting:\n",
    "```python\n",
    "index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    " ])\n",
    "```\n",
    "\n",
    "### Note on optimization\n",
    "\n",
    "To improve througput, we are going to add `async_req=True` parameter. \n",
    "Upsert will return futures that we need to wait.\n",
    "\n",
    "Optimization tips:\n",
    "1. Deploy application at the same region\n",
    "2. Use batching upsert\n",
    "3. Use parallelized upsert\n",
    "4. Use GRPCIndex, but make sure to add backoff for throttling\n",
    "5. Use namespaces and metadata filtering\n",
    "6. Avoid quotas and limits: https://docs.pinecone.io/reference/quotas-and-limits\n",
    "\n",
    "Optimized upsert code:\n",
    "```python\n",
    "future = index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    "    ],\n",
    "    async_req=True\n",
    ")\n",
    "....\n",
    "future.get() # wait for upsert to complete\n",
    "```\n",
    "\n",
    "For scale-up and optimizations make sure to read: : https://docs.pinecone.io/guides/operations/performance-tuning#increasing-throughput  \n",
    "Metadata filtering: Metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pinecone import Index\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 100) -> None:\n",
    "    futures = []\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        i_end = min(i + batch_size, len(ds))\n",
    "        batch = ds.select(range(i, i_end))\n",
    "\n",
    "        # The upsert requires the vectors to be a list of tuples (id, vector, metadata)\n",
    "        # Example: [(id1, vector1, metadata1), (id2, vector2, metadata2), ...]\n",
    "        # TODO: Select the id, embeddings, and metadata from the batch\n",
    "        ids = batch[\"id\"]\n",
    "        embeddings = batch[\"embeddings\"]\n",
    "        metadata = batch[\"metadata\"]\n",
    "\n",
    "        # TODO: Upsert the vectors to the Pinecone index asynchronously\n",
    "        # Use `index.upsert(vectors=[(id1, vector1, metadata1), (id2, vector2, metadata2)], async_req=True)`\n",
    "        futures.append(\n",
    "            index.upsert(vectors=list(zip(ids, embeddings, metadata)), async_req=True)\n",
    "        )\n",
    "\n",
    "    # Wait for all the upserts to complete\n",
    "    for future in tqdm(futures, total=len(futures), desc=\"Upsert to Pinecone\"):\n",
    "        future.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upsert to Pinecone: 100%|██████████| 25/25 [00:02<00:00,  9.95it/s]\n"
     ]
    }
   ],
   "source": [
    "upsert_batch(dataset.select(range(500)), index, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 500}},\n",
      " 'total_vector_count': 500}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upsert to Pinecone: 100%|██████████| 400/400 [02:44<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "upsert_batch(dataset.select(range(40_000)), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 42500}},\n",
      " 'total_vector_count': 42500}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has been done with data set?\n",
    "\n",
    "![chunking-dataset.png](https://i.ibb.co/9wV70Q7/chunking-dataset.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "#### Chunking Strategies\n",
    "1. Character split (with overlapping)\n",
    "2. Recursive character split\n",
    "3. Document specific splitting\n",
    "4. Semantic Chunking\n",
    "5. Agentic?\n",
    "6. More?\n",
    "\n",
    "Introduction to chunking: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![done-next.png](https://i.ibb.co/QYC9nrb/done-next.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve against the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![semantic-search.png](https://i.ibb.co/2dWGRHn/semantic-search.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we inserted everythong to Pinecone, let's query it. The input is query text and the output is the top similar chunks.\n",
    "Steps: \n",
    "1. Encode the input text to generate embeddings \n",
    "2. Call Pinecone's query function to retrieve top K similar results\n",
    "\n",
    "### Encode (create embeddings)\n",
    "\n",
    "Create embeddings:\n",
    "```python\n",
    "openai_client.embeddings.create(model=\"embedding-model-name\", input=\"Text to create embeddings\")\n",
    "```\n",
    "\n",
    "Embedding API Ref: https://platform.openai.com/docs/api-reference/embeddings  \n",
    "Query API Ref: https://docs.pinecone.io/reference/api/data-plane/query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def encode(\n",
    "    text: str, openai_client: OpenAI, model: str = \"text-embedding-3-small\"\n",
    ") -> List[float]:\n",
    "    # TODO: create embeddings using OpenAI API\n",
    "    # 1. Call openai_client.embeddings.create with the model and text\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02581052854657173, 0.03242715075612068, 0.019921164959669113, -0.021119002252817154, -0.036020658910274506, 0.023999514058232307, -0.05059434100985527, 0.05886511504650116, 0.024940671399235725, -0.006969555746763945]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "res = encode(\"What are key benefits of mistral 7B?\", openai)\n",
    "print(res[:10])\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Pinecone\n",
    "\n",
    "Query Pinecone:\n",
    "We are going to retrieve metadata, but not vectors itself\n",
    "```python\n",
    "index.query(\n",
    "        vector=QUERY_EMBEDDINGS, top_k=TOP_K, include_metadata=True, include_values=False\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import QueryResponse\n",
    "\n",
    "\n",
    "def semantic_search(\n",
    "    query: str, index: Index, openai_client: OpenAI, top_k: int = 10\n",
    ") -> QueryResponse:\n",
    "    # TODO: Implement semantic search using the OpenAI API and Pinecone index\n",
    "    # 1. Reuse `encode` function to create embeddings for the query\n",
    "    # 2. Use `index.query` to query the index with the vector and top_k and set include_metadata=True\n",
    "    query_embedding = encode(query, openai_client)\n",
    "    ret = index.query(\n",
    "        vector=query_embedding, top_k=top_k, include_metadata=True, include_values=False\n",
    "    )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "# Introduction\n"
     ]
    }
   ],
   "source": [
    "res = semantic_search(\"What is Mistral 7B?\", index, openai)\n",
    "print(res.matches[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augment and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![workflow-rag-simple.png](https://i.ibb.co/p0gwY23/workflow-rag-simple.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final response\n",
    "\n",
    "Combine all pieces together:\n",
    "1. Perform a semantic search on input query\n",
    "2. Build a context (prompt) for a LLM\n",
    "3. Call LLM to generate a final response\n",
    "4. Return a final response and retrieved context\n",
    "\n",
    "The relevant context can be found in metadata, you can use:\n",
    "1. `title` - a Paper title\n",
    "2. `published` - a publish date\n",
    "3. `primart_category` \n",
    "4. `summary` - a paper summary\n",
    "5. `text` - a chunk text - this is the most useful info to build a context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def from_metadata(metadata: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "***\n",
    "    Title: {metadata['title']}\n",
    "    Authors: {metadata['authors'][:5]}\n",
    "    Published: {metadata['published']}\n",
    "    Primary category: {metadata['primary_category']}\n",
    "    Paper summary: {metadata['summary']}\n",
    "    Text: {metadata['text']}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(query: str, query_results: QueryResponse) -> str:\n",
    "    context = \"\\n\".join(\n",
    "        [from_metadata(result.metadata) for result in query_results.matches]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "The answers should be clear, easy to understand, complete and comprehensive.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rag(query: str, index: Index, openai_client: OpenAI, top_k: int = 5) -> Tuple[str, QueryResponse]:\n",
    "    # TODO: Wire all RAG pieces together to generate a response (retrieve, augment, generate)\n",
    "    # 1. [RETRIEVE]: Reuse `semantic_search` function to get the top_k results\n",
    "    # 2. [AUGMENT]: Use `get_prompt` function to generate the prompt (context + question)\n",
    "    # 3. [GENERATE]: Use `llm_completion` function to generate the response\n",
    "    # 4. Return the response and query_results\n",
    "    query_results = semantic_search(query, index, openai_client, top_k=top_k)\n",
    "    prompt = get_prompt(query, query_results)\n",
    "    response = llm_completion(prompt, openai_client)\n",
    "    return response, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B are:\n",
       "\n",
       "1. **Superior Performance**: Mistral 7B is engineered for superior performance, outperforming the Llama 2 13B model across all evaluated benchmarks and surpassing the Llama 1 34B model in reasoning, mathematics, and code generation.\n",
       "\n",
       "2. **Efficiency**: Mistral 7B is designed to be efficient, balancing high performance with reduced computational costs and inference latency. This efficiency is crucial for deployment in practical, real-world scenarios.\n",
       "\n",
       "3. **Innovative Attention Mechanisms**: Mistral 7B leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length effectively with reduced inference costs. These attention mechanisms contribute to the model's enhanced performance and efficiency.\n",
       "\n",
       "4. **Ease of Deployment**: Mistral 7B is released under the Apache 2.0 license, accompanied by a reference implementation that facilitates easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure. Integration with tools like vLLM inference server and Hugging Face is streamlined for easier deployment.\n",
       "\n",
       "5. **Fine-Tuning Capabilities**: Mistral 7B can be fine-tuned across a myriad of tasks, showcasing adaptability and superior performance. Models fine-tuned from Mistral 7B, such as Mistral 7B -- Instruct, have demonstrated significant improvements over existing models in both human and automated benchmarks.\n",
       "\n",
       "Overall, Mistral 7B aims to provide a high-performing, efficient language model that can be used in a wide range of real-world applications, offering advancements in performance, efficiency, and ease of deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What are key benefits of Mistral 7B?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>3.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
       "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
       "# 2 Architectural details\n",
       "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>1.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Abstract\n",
       "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
       "# Introduction</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Mistral 7B** is a 7-billion-parameter language model engineered for superior performance and efficiency. It outperforms Llama 2 13B across all evaluated benchmarks and Llama 1 34B in reasoning, mathematics, and code generation. Mistral 7B leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length efficiently. Additionally, Mistral 7B provides a model fine-tuned to follow instructions, named Mistral 7B -- Instruct, which surpasses the Llama 2 13B -- Chat model on both human and automated benchmarks. The models are released under the Apache 2.0 license.\n",
       "\n",
       "- **Kosmos-1** is a Multimodal Large Language Model (MLLM) designed to perceive general modalities, learn in context (few-shot learning), and follow instructions (zero-shot learning). It is trained from scratch on web-scale multimodal corpora, including text and images, image-caption pairs, and text data. Kosmos-1 excels in language understanding, generation, OCR-free NLP, perception-language tasks, and vision tasks. It can benefit from cross-modal transfer and has a dataset for diagnosing nonverbal reasoning capability.\n",
       "\n",
       "- **Difference between Mistral and Kosmos**: Mistral 7B is a language model focused on performance and efficiency, leveraging specific attention mechanisms for inference. On the other hand, Kosmos-1 is a Multimodal Large Language Model that integrates perception with language models, excelling in various tasks across different modalities and learning contexts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = rag(\"The difference between Mistral and Kosmos?\", index, openai, top_k=20)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Topics\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "TBA - Add mini-agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Once we explored the simple components of RAG, we can use existing frameworks to build them.  \n",
    "\n",
    "Langchain: https://www.langchain.com/langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index, embedding=embeddings, text_key=\"text\", pinecone_api_key=PINECONE_API_KEY, index_name=INDEX_NAME,)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "langchain_rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are all benefits of using Mistral 7B?',\n",
       " 'answer': 'The benefits of using Mistral 7B include being released under the Apache 2.0 license, easy deployment on cloud platforms, integration with Hugging Face, ease of fine-tuning across tasks, and superior performance compared to other models.\\n',\n",
       " 'sources': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_rag.invoke(\"What are all benefits of using Mistral 7B?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune vs RAG vs Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag-vs-prompt-vs-finetune.png](https://i.ibb.co/C8rvnTY/Screenshot-2024-05-30-at-21-17-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key difference between Mistral 7B and Palm lies in their respective focuses and capabilities:\n",
       "\n",
       "- **Mistral 7B**:\n",
       "  - **Focus**: Mistral 7B is engineered for superior performance and efficiency in natural language processing tasks, including reasoning, mathematics, and code generation.\n",
       "  - **Model Features**: It leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length efficiently.\n",
       "  - **Fine-Tuning**: Mistral 7B offers a model fine-tuned to follow instructions, Mistral 7B -- Instruct, which surpasses the Llama 2 13B -- Chat model on both human and automated benchmarks.\n",
       "  - **License**: Mistral 7B models are released under the Apache 2.0 license.\n",
       "\n",
       "- **Palm**:\n",
       "  - **Focus**: Palm models aim to be state-of-the-art language models with better multilingual and reasoning capabilities, as well as improved compute efficiency.\n",
       "  - **Training**: Palm 2 is a Transformer-based model trained using a mixture of objectives to achieve high-quality performance on downstream tasks.\n",
       "  - **Inference**: Palm models exhibit faster and more efficient inference compared to their predecessor, Palm.\n",
       "  - **Responsibility**: Palm models demonstrate stable performance on responsible AI evaluations and enable inference-time control over toxicity without additional overhead.\n",
       "\n",
       "In summary, Mistral 7B is focused on performance and efficiency in NLP tasks, while Palm models emphasize multilingual capabilities, reasoning, and responsible AI practices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\n",
    "    \"The key difference between Mistral 7B and Palm?\", index, openai, top_k=20\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Time\n",
    "\n",
    "#### Query Time\n",
    "\n",
    "#### Evaluations\n",
    "\n",
    "#### Agentic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Better embeddings\n",
    "# Better chunking\n",
    "# Better indexing\n",
    "# Better query understanding\n",
    "# History management\n",
    "# Evaluation\n",
    "# Ranking (Context Limitation)\n",
    "# Guardrails\n",
    "# Frameworks (langchain / canopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
