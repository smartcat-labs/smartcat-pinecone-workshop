{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title.png](https://i.ibb.co/2KmT38V/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![objectives.png](https://i.ibb.co/fxbWnNQ/objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "!   pip install -qU \\\n",
    "    openai==1.30 \\\n",
    "    \"pinecone-client[grpc]==4.1.0\" \\\n",
    "    datasets==2.19 \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def chunk_display_html(chunk: Dict[str, str]) -> str:\n",
    "    html_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    table {{\n",
    "        font-family: arial, sans-serif;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    td, th {{\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Key</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Title</td>\n",
    "            <td>{title}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>doi</td>\n",
    "            <td>{doi}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Chunk ID</td>\n",
    "            <td>{chunk_id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>chunk</td>\n",
    "            <td>{chunk}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>id</td>\n",
    "            <td>{id}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Summary</td>\n",
    "            <td>{summary}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Source</td>\n",
    "            <td>{source}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Authors</td>\n",
    "            <td>{authors}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>{categories}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Comment</td>\n",
    "            <td>{comment}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Journal Reference</td>\n",
    "            <td>{journal_ref}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Primary Category</td>\n",
    "            <td>{primary_category}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Published</td>\n",
    "            <td>{published}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Updated</td>\n",
    "            <td>{updated}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>References</td>\n",
    "            <td>{references}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # Format the HTML with the generated rows\n",
    "    html_output = html_template.format(\n",
    "        doi=chunk.get(\"doi\", \"N/A\"),\n",
    "        chunk_id=chunk.get(\"chunk-id\", \"N/A\"),\n",
    "        chunk=chunk.get(\"chunk\", \"N/A\"),\n",
    "        id=chunk.get(\"id\", \"N/A\"),\n",
    "        title=chunk.get(\"title\", \"N/A\"),\n",
    "        summary=chunk.get(\"summary\", \"N/A\"),\n",
    "        source=chunk.get(\"source\", \"N/A\"),\n",
    "        authors=chunk.get(\"authors\", \"N/A\"),\n",
    "        categories=chunk.get(\"categories\", \"N/A\"),\n",
    "        comment=chunk.get(\"comment\", \"N/A\"),\n",
    "        journal_ref=chunk.get(\"journal_ref\", \"N/A\"),\n",
    "        primary_category=chunk.get(\"primary_category\", \"N/A\"),\n",
    "        published=chunk.get(\"published\", \"N/A\"),\n",
    "        updated=chunk.get(\"updated\", \"N/A\"),\n",
    "        references=chunk.get(\"references\", \"N/A\"),\n",
    "    )\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_retrieved_context(context_response):\n",
    "    # HTML template for the main container and individual tables\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        .container {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            margin: 10px;\n",
    "            padding: 10px;\n",
    "            border: 1px solid #dddddd;\n",
    "        }}\n",
    "        table {{\n",
    "            font-family: arial, sans-serif;\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }}\n",
    "        td, th {{\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            {tables}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to generate HTML table for a single dictionary\n",
    "    def generate_table_for_dict(data):\n",
    "        rows = \"\\n\".join(\n",
    "            \"<tr><td>{key}</td><td>{value}</td></tr>\".format(\n",
    "                key=key, value=value if value is not None else \"N/A\"\n",
    "            )\n",
    "            for key, value in data.items()\n",
    "        )\n",
    "        table_html = \"\"\"\n",
    "        <div class=\"table-container\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Key</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                {rows}\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\".format(\n",
    "            rows=rows\n",
    "        )\n",
    "        return table_html\n",
    "\n",
    "    # Generate HTML tables for all dictionaries in the list\n",
    "    tables = \"\\n\".join(\n",
    "        generate_table_for_dict(data[\"metadata\"]) for data in context_response\n",
    "    )\n",
    "\n",
    "    # Format the main HTML with the generated tables\n",
    "    html_output = html_template.format(tables=tables)\n",
    "\n",
    "    # Display the HTML in an IPython notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "\n",
    "def display_markdown(content: str) -> None:\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-away-rag.png](https://i.ibb.co/Y288TWR/step-away-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the OpenAI API key and instantiate the OpenAI clinet.\n",
    "\n",
    "Copy this openai API key `sk-XXXXXXXXXX` into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text using OpenAI GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "API reference: https://platform.openai.com/docs/api-reference/chat/create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, openai_client: OpenAI, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Output is markdown\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of Germany is Berlin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"What is the capital of Germany?\", openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 25th person to land on the moon was Harrison Schmitt. He was part of the Apollo 17 mission in December 1972 and was the only geologist to walk on the moon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"Who is 25th person that landed on the moon?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hallucinations-problem.png](https://i.ibb.co/gMvNZC6/hallucinations-problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Mistral 7B is a powerful and versatile wind turbine that offers several key benefits:\n",
       "\n",
       "1. **High Efficiency**: The Mistral 7B is designed to maximize energy production with its high efficiency blades and generator, ensuring that you get the most out of the available wind resources.\n",
       "\n",
       "2. **Robust Construction**: This wind turbine is built to withstand harsh weather conditions and has a durable construction that ensures long-term reliability and performance.\n",
       "\n",
       "3. **Quiet Operation**: The Mistral 7B is designed to operate quietly, making it suitable for residential areas or locations where noise levels need to be minimized.\n",
       "\n",
       "4. **Easy Installation**: With its user-friendly design and easy installation process, the Mistral 7B can be set up quickly and efficiently, saving time and effort.\n",
       "\n",
       "5. **Remote Monitoring**: Some models of the Mistral 7B come with remote monitoring capabilities, allowing you to track the performance of the turbine and troubleshoot any issues from a distance.\n",
       "\n",
       "Overall, the Mistral 7B offers a combination of efficiency, durability, and ease of use, making it a popular choice for renewable energy generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generate(\"What are key benefits of mistral 7B?\", openai, model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge-cutoff.png](https://i.ibb.co/ccccpxZ/knowledge-cutoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B include:\n",
       "\n",
       "1. Superior performance and efficiency compared to other models like Llama 2 (13B) and Llama 1 (34B).\n",
       "2. Outperforming other models in reasoning, mathematics, and code generation.\n",
       "3. Leveraging grouped-query attention (GQA) for faster inference.\n",
       "4. Using sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost.\n",
       "5. Providing a fine-tuned model, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_example = \"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "\n",
    "Context:\n",
    "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "\n",
    "Question: What are key benefits of Mistral 7B?\n",
    "\"\"\"\n",
    "display_markdown(generate(context_example, openai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![why-rag.png](https://i.ibb.co/KF3xr64/why-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how-to-build-rag.png](https://i.ibb.co/wgXqfFv/how-to-build-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![build-knowledge-base.png](https://i.ibb.co/dGnjrCk/build-knowledge-base.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone \n",
    "\n",
    "Enter the Pinecone API key inside the prompt and create a Pinecone client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_REGION = \"us-east-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "INDEX_NAME = \"pinecone-workshop-1\"\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'pinecone-worshop-1-2kw20wn.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-worshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'pinecone-workshop-1-2kw20wn.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'pinecone-workshop-1',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC\n",
    "#from pinecone import Pinecone\n",
    "\n",
    "pinecone = PineconeGRPC(api_key=PINECONE_API_KEY)\n",
    "# pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index\n",
    "\n",
    "More info o serverless: https://docs.pinecone.io/reference/architecture/serverless-architecture#overview  \n",
    "Ref API: https://docs.pinecone.io/reference/api/control-plane/create_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0}},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Check if the index already exists and delete it\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# Create a new index with the specified name, dimension, metric, and spec\n",
    "# Docs: https://docs.pinecone.io/reference/api/control-plane/create_index\n",
    "pinecone.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=VECTOR_DIMENSIONS,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
    ")\n",
    "\n",
    "# Create a new Index reference object with the specified name and pool_threads=30\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use a sample of 1000 AI papers that can be found here: https://huggingface.co/datasets/smartcat/ai-arxiv2-chunks-embedded \n",
    "\n",
    "The data set is already chunked and encoded using `text-embeddings-3-small` so we can just load and upsert it to the Pinecone.\n",
    "\n",
    "If you want to play with chunking strategies and embeddings, you can find the full data set here: https://huggingface.co/datasets/jamescalam/ai-arxiv2\n",
    "\n",
    "Dataset API reference: https://huggingface.co/docs/datasets/en/index  \n",
    "Slicing and indexing: https://huggingface.co/docs/datasets/en/access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'embeddings', 'metadata'],\n",
       "    num_rows: 79782\n",
       "})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"smartcat/ai-arxiv2-chunks-embedded\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "    table {\n",
       "        font-family: arial, sans-serif;\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    td, th {\n",
       "        border: 1px solid #dddddd;\n",
       "        text-align: left;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Key</th>\n",
       "            <th>Value</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Title</td>\n",
       "            <td>Foundations of Vector Retrieval</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>doi</td>\n",
       "            <td>2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Chunk ID</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>chunk</td>\n",
       "            <td>4 2 0 2\n",
       "n a J 7 1 ] S D . s c [\n",
       "1 v 0 5 3 9 0 . 1 0 4 2 : v i X r a\n",
       "Sebastian Bruch\n",
       "# Foundations of Vector Retrieval\n",
       "# Preface\n",
       "We are witness to a few years of remarkable developments in Artificial Intelligence with the use of advanced machine learning algorithms, and in particular, deep learning. Gargantuan, complex neural networks that can learn through self-supervisionâand quickly so with the aid of special- ized hardwareâtransformed the research landscape so dramatically that, incremental overnight it seems, many fields experienced not the usual, progress, but rather a leap forward. Machine translation, natural language understanding, information retrieval, recommender systems, and computer vision are but a few examples of research areas that have had to grapple with the shock. Countless other disciplines beyond computer science such as robotics, biology, and chemistry too have benefited from deep learning.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>id</td>\n",
       "            <td>2401.09350#0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Summary</td>\n",
       "            <td>Vectors are universal mathematical objects that can represent text, images,\n",
       "speech, or a mix of these data modalities. That happens regardless of whether\n",
       "data is represented by hand-crafted features or learnt embeddings. Collect a\n",
       "large enough quantity of such vectors and the question of retrieval becomes\n",
       "urgently relevant: Finding vectors that are more similar to a query vector.\n",
       "This monograph is concerned with the question above and covers fundamental\n",
       "concepts along with advanced data structures and algorithms for vector\n",
       "retrieval. In doing so, it recaps this fascinating topic and lowers barriers of\n",
       "entry into this rich area of research.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Source</td>\n",
       "            <td>http://arxiv.org/pdf/2401.09350</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Authors</td>\n",
       "            <td>Sebastian Bruch</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Categories</td>\n",
       "            <td>cs.DS, cs.IR</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Comment</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Journal Reference</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Primary Category</td>\n",
       "            <td>cs.DS</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Published</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Updated</td>\n",
       "            <td>20240117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>References</td>\n",
       "            <td>[]</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_display_html(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.020014504, -0.013545036, 0.04353385, -0.0029185074, 0.03552278, -0.034943033, 0.013927143, 0.06566971, -0.06888468, 0.026971487]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0][\"embeddings\"]))\n",
    "print(dataset[0][\"embeddings\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authors',\n",
       " 'chunk_id',\n",
       " 'doc_id',\n",
       " 'primary_category',\n",
       " 'published',\n",
       " 'source',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'title']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset[0][\"metadata\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has been done with data set?\n",
    "\n",
    "![chunking-dataset.png](https://i.ibb.co/9wV70Q7/chunking-dataset.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "#### Chunking Strategies\n",
    "1. Character split (with overlapping)\n",
    "2. Recursive character split\n",
    "3. Document specific splitting\n",
    "4. Semantic Chunking\n",
    "5. Agentic?\n",
    "6. More?\n",
    "\n",
    "Introduction to chunking: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data upsert to Pinecone\n",
    "\n",
    "Let insert data to the Pinecone in batches. \n",
    "\n",
    "From our data set we need 3 columns:\n",
    "1. `id` - the ID of the chunk we want to insert\n",
    "2. `embeddings` - contains a vector embedding of the chunk. It uses `text-embeddings-3-small`\n",
    "3. `metadata` - a dictinary with additional data about the chunk. \n",
    "\n",
    "The code for upserting:\n",
    "```python\n",
    "index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    " ])\n",
    "```\n",
    "\n",
    "### Note on optimization\n",
    "\n",
    "To improve througput, we are going to add `async_req=True` parameter. \n",
    "Upsert will return futures that we need to wait.\n",
    "\n",
    "Optimization tips:\n",
    "1. Deploy application at the same region\n",
    "2. Use batching upsert\n",
    "3. Use parallelized upsert\n",
    "4. Use GRPCIndex, but make sure to add backoff for throttling\n",
    "5. Use namespaces and metadata filtering\n",
    "6. Avoid quotas and limits: https://docs.pinecone.io/reference/quotas-and-limits\n",
    "\n",
    "Optimized upsert code:\n",
    "```python\n",
    "future = index.upsert(vectors=[ \n",
    "    (id1, vector1, metadata1),\n",
    "    (id2, vector2, metadata2),\n",
    "    ....\n",
    "    ],\n",
    "    async_req=True\n",
    ")\n",
    "....\n",
    "future.get() # wait for upsert to complete\n",
    "```\n",
    "\n",
    "For scale-up and optimizations make sure to read: : https://docs.pinecone.io/guides/operations/performance-tuning#increasing-throughput  \n",
    "Metadata filtering: Metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pinecone import Index\n",
    "\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 100) -> None:\n",
    "    futures = []\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        i_end = min(i + batch_size, len(ds))\n",
    "        batch = ds.select(range(i, i_end))\n",
    "\n",
    "        # Select the id, embeddings, and metadata from the batch\n",
    "        ids = batch[\"id\"]\n",
    "        embeddings = batch[\"embeddings\"]\n",
    "        metadata = batch[\"metadata\"]\n",
    "\n",
    "        # Upsert the vectors to the Pinecone index asynchronously\n",
    "        # Use `index.upsert(vectors=[(id1, vector1, metadata1), (id2, vector2, metadata2)], async_req=True)`\n",
    "        futures.append(\n",
    "            index.upsert(vectors=list(zip(ids, embeddings, metadata)), async_req=True)\n",
    "        )\n",
    "\n",
    "    # Wait for all the upserts to complete\n",
    "    for future in tqdm(futures, total=len(futures), desc=\"Waiting for upserts to complete\"):\n",
    "        future.get()\n",
    "\n",
    "\n",
    "def upsert_batch(ds: datasets.Dataset, index: Index, batch_size: int = 200) -> None:\n",
    "    df = ds.to_pandas()\n",
    "    df = df[[\"id\", \"embeddings\", \"metadata\"]]\n",
    "    df.rename(columns={\"embeddings\": \"values\"}, inplace=True)\n",
    "    index.upsert_from_dataframe(df, batch_size=batch_size, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b9490155314e6ebf74ec9f686199ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sending upsert requests:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dce409b04414d22862e34871308c158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting async responses:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsert_batch(dataset.select(range(500)), index, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 500}},\n",
      " 'total_vector_count': 500}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625f150f95204b30be332d14730593ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sending upsert requests:   0%|          | 0/79782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ec883c557345e387360f0e945b3bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting async responses:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsert_batch(dataset, index, batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![done-next.png](https://i.ibb.co/QYC9nrb/done-next.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve against the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![semantic-search.png](https://i.ibb.co/2dWGRHn/semantic-search.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we inserted everythong to Pinecone, let's query it. The input is query text and the output is the top similar chunks.\n",
    "Steps: \n",
    "1. Encode the input text to generate embeddings \n",
    "2. Call Pinecone's query function to retrieve top K similar results\n",
    "\n",
    "Embedding API Ref: https://platform.openai.com/docs/api-reference/embeddings  \n",
    "Query API Ref: https://docs.pinecone.io/reference/api/data-plane/query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def encode(\n",
    "    text: str, openai_client: OpenAI, model: str = \"text-embedding-3-small\"\n",
    ") -> List[float]:\n",
    "    # Use the OpenAI API to encode the text\n",
    "    # Docs: https://platform.openai.com/docs/api-reference/embeddings\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model, input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02581052854657173, 0.03242715075612068, 0.019921164959669113, -0.021119002252817154, -0.036020658910274506, 0.023999514058232307, -0.05059434100985527, 0.05886511504650116, 0.024940671399235725, -0.006969555746763945]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "res = encode(\"What are key benefits of mistral 7B?\", openai)\n",
    "print(res[:10])\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import QueryResponse\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    query: str, index: Index, openai_client: OpenAI, top_k: int = 10\n",
    ") -> QueryResponse:\n",
    "    # Encode the query using the OpenAI API\n",
    "    query_embedding = encode(query, openai_client)\n",
    "\n",
    "    # Use the Pinecone index to query the encoded vector\n",
    "    # Docs: https://docs.pinecone.io/reference/api/data-plane/query\n",
    "    ret = index.query(\n",
    "        vector=query_embedding, top_k=top_k, include_metadata=True, include_values=False\n",
    "    )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "# Introduction\n"
     ]
    }
   ],
   "source": [
    "res = retrieve(\"What is Mistral 7B?\", index, openai)\n",
    "print(res.matches[0].metadata['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augment and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)\n",
    "\n",
    "![workflow-rag-simple.png](https://i.ibb.co/p0gwY23/workflow-rag-simple.png)\n",
    "\n",
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a final response\n",
    "\n",
    "Combine all pieces together:\n",
    "1. Perform a semantic search on input query\n",
    "2. Build a context (prompt) for a LLM\n",
    "3. Call LLM to generate a final response\n",
    "4. Return a final response and retrieved context\n",
    "\n",
    "The relevant context can be found in metadata, you can use:\n",
    "1. `title` - a Paper title\n",
    "2. `published` - a publish date\n",
    "3. `primart_category` \n",
    "4. `summary` - a paper summary\n",
    "5. `text` - a chunk text - this is the most useful info to build a context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def from_metadata(metadata: Dict) -> str:\n",
    "    return f\"\"\"\n",
    "***\n",
    "    Title: {metadata['title']}\n",
    "    Authors: {metadata['authors'][:5]}\n",
    "    Published: {metadata['published']}\n",
    "    Primary category: {metadata['primary_category']}\n",
    "    Paper summary: {metadata['summary']}\n",
    "    Text: {metadata['text']}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def augment(query: str, query_results: QueryResponse) -> str:\n",
    "    context = \"\\n\".join(\n",
    "        [from_metadata(result.metadata) for result in query_results.matches]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "Answer the question based on the following context. If you don't can't find the answer, tell I don't know.\n",
    "The answers should be clear, easy to understand, complete and comprehensive.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rag(query: str, index: Index, openai_client: OpenAI, top_k: int = 5) -> Tuple[str, QueryResponse]:\n",
    "    # 1. [RETRIEVE]: Reuse `semantic_search` function to get the top_k results\n",
    "    # 2. [AUGMENT]: Use `get_prompt` function to generate the prompt (context + question)\n",
    "    # 3. [GENERATE]: Use `llm_completion` function to generate the response\n",
    "    # 4. Return the response and query_results\n",
    "    query_results = retrieve(query, index, openai_client, top_k=top_k)\n",
    "    prompt = augment(query, query_results)\n",
    "    response = generate(prompt, openai_client)\n",
    "    return response, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key benefits of Mistral 7B are:\n",
       "\n",
       "1. **Superior Performance**: Mistral 7B is engineered for superior performance, outperforming the Llama 2 13B model across all evaluated benchmarks and surpassing the Llama 1 34B model in reasoning, mathematics, and code generation.\n",
       "\n",
       "2. **Efficiency**: Mistral 7B is designed to be efficient, balancing high performance with reduced computational costs and inference latency. It demonstrates that a carefully designed language model can deliver high performance while maintaining efficient inference.\n",
       "\n",
       "3. **Innovative Attention Mechanisms**: Mistral 7B leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length effectively with reduced inference cost. These attention mechanisms contribute to the enhanced performance and efficiency of Mistral 7B.\n",
       "\n",
       "4. **Ease of Deployment and Fine-Tuning**: Mistral 7B is released under the Apache 2.0 license, accompanied by a reference implementation for easy deployment on cloud platforms. It is also crafted for ease of fine-tuning across a myriad of tasks, making it adaptable for various real-world applications.\n",
       "\n",
       "5. **Outperforming Competing Models**: Mistral 7B surpasses the Llama 2 13B model in both human and automated benchmarks, showcasing its ability to outperform existing models in various tasks such as chat modeling and following instructions.\n",
       "\n",
       "Overall, Mistral 7B aims to provide a high-performing language model that is efficient, easy to deploy, and adaptable for a wide range of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What are key benefits of Mistral 7B?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
       "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
       "# 2 Architectural details\n",
       "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length</td></tr>\n",
       "<tr><td>chunk_id</td><td>3.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>Abstract\n",
       "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
       "# Introduction</td></tr>\n",
       "<tr><td>chunk_id</td><td>1.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A chain of thought refers to a coherent flow of sentences that reveals the premises and conclusion of a reasoning problem. It clearly decomposes a multi-hop reasoning task into intermediate steps instead of solving the task in a black-box way. The chain of thought can be the step-by-step thought process before arriving at the final answer or explanations that come after the answer. In the context of language models, a chain of thought is a series of intermediate reasoning steps that help improve the ability of large language models to perform complex reasoning tasks. It allows models to decompose multi-step problems into intermediate steps, providing a more interpretable window into the behavior of the model and enabling additional computation to be allocated to problems that require more reasoning steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"What is chain of thoughts?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To apply Large Language Models (LLMs) to recommender engines, several key strategies and techniques can be employed based on the context provided:\n",
       "\n",
       "1. **LLMs as Recommendation Models**:\n",
       "   - **Zero-Shot Paradigm**: LLMs can be prompted to complete the recommendation task without parameter tuning. This approach involves prompt engineering methods like recency-focused and in-context learning to improve recommendation performance and mitigate model biases.\n",
       "   - **Instruction Tuning**: Specializing LLMs for personalized recommendations through instruction tuning is another effective method. High-quality instruction data is crucial for adapting LLMs to recommendation tasks. This data can be constructed based on user-item interactions with heuristic templates to enhance instruction diversity.\n",
       "\n",
       "2. **LLM-Enhanced Recommendation Models**:\n",
       "   - **Incorporating Universal Knowledge**: LLMs can leverage the universal knowledge encoded in them to enhance traditional recommender systems. This can be achieved through various approaches:\n",
       "     - Inferring users' potential intentions from historical interaction data and using them to improve item retrieval.\n",
       "     - Encoding side information of items and users (e.g., descriptions, reviews) using LLMs to provide more informative representations for traditional recommender systems.\n",
       "     - Adopting distillation-like methods to transfer LLM capacities to improve traditional recommenders by aligning hidden states of LLMs and traditional models through joint training.\n",
       "\n",
       "3. **LLMs as Recommendation Simulators**:\n",
       "   - LLMs can also be utilized as recommendation simulators to simulate potential user instructions in various scenarios like product search and personalized recommendations. This approach can help in enhancing the recommendation process by incorporating diverse user behaviors and preferences.\n",
       "\n",
       "4. **Addressing Challenges**:\n",
       "   - **Semantic Gap**: Despite the potential of LLMs, challenges such as understanding personalized user behaviors and domain-specific collaborative semantics need to be addressed. Instruction tuning and vocabulary extension with semantic identifiers can help bridge this gap.\n",
       "   - **Efficiency**: To deploy LLMs effectively in real-world recommender systems, techniques like efficient tuning, quantization methods, and improved context length extension should be explored to enhance inference speed and reduce memory overhead.\n",
       "\n",
       "By implementing these strategies and addressing the associated challenges, LLMs can be effectively applied to recommender engines to enhance recommendation performance and user experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\"How to apply LLMs to recommender engines?\", index, openai)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>LLM-enhanced Recommendation Models. In addition to instructing LLMs to directly provide recommendations, re- searchers also propose leveraging the universal knowledge encoded in LLMs to improve traditional recommender sys- tems. Existing approaches in this line can be divided into three main categories. The first category employs LLMs to infer usersâ potential intention from their historical interac- tion data. Furthermore, traditional recommendation/search models employ the inferred intentions to improve the re- trieval of relevant items [812, 813]. Additionally, several studies explore the use of LLMs as feature encoders. They employ LLMs to encode the side information of items and\n",
       "75\n",
       "users (e.g., itemâs descriptions and userâs reviews), thus de- riving more informative representations of users and items. These representations are then fed into traditional recom- mender systems as augmented input [814, 815]. As an- other alternative approach, several studies [816, 817] adopt a distillation-like way to transfer LLMâs capacities (e.g., semantic encoding) to improve traditional recommenders (i.e., small models). Specially, they align the hidden states of LLMs and traditional recommendation models via joint training. After training, since only the enhanced small model will be deployed online, it can avoid the huge over- head of LLMs in online service.</td></tr>\n",
       "<tr><td>chunk_id</td><td>481.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2303.18223</td></tr>\n",
       "<tr><td>summary</td><td>Language is essentially a complex, intricate system of human expressions\n",
       "governed by grammatical rules. It poses a significant challenge to develop\n",
       "capable AI algorithms for comprehending and grasping a language. As a major\n",
       "approach, language modeling has been widely studied for language understanding\n",
       "and generation in the past two decades, evolving from statistical language\n",
       "models to neural language models. Recently, pre-trained language models (PLMs)\n",
       "have been proposed by pre-training Transformer models over large-scale corpora,\n",
       "showing strong capabilities in solving various NLP tasks. Since researchers\n",
       "have found that model scaling can lead to performance improvement, they further\n",
       "study the scaling effect by increasing the model size to an even larger size.\n",
       "Interestingly, when the parameter scale exceeds a certain level, these enlarged\n",
       "language models not only achieve a significant performance improvement but also\n",
       "show some special abilities that are not present in small-scale language\n",
       "models. To discriminate the difference in parameter scale, the research\n",
       "community has coined the term large language models (LLM) for the PLMs of\n",
       "significant size. Recently, the research on LLMs has been largely advanced by\n",
       "both academia and industry, and a remarkable progress is the launch of ChatGPT,\n",
       "which has attracted widespread attention from society. The technical evolution\n",
       "of LLMs has been making an important impact on the entire AI community, which\n",
       "would revolutionize the way how we develop and use AI algorithms. In this\n",
       "survey, we review the recent advances of LLMs by introducing the background,\n",
       "key findings, and mainstream techniques. In particular, we focus on four major\n",
       "aspects of LLMs, namely pre-training, adaptation tuning, utilization, and\n",
       "capacity evaluation. Besides, we also summarize the available resources for\n",
       "developing LLMs and discuss the remaining issues for future directions.</td></tr>\n",
       "<tr><td>published</td><td>20230331</td></tr>\n",
       "<tr><td>title</td><td>A Survey of Large Language Models</td></tr>\n",
       "<tr><td>doc_id</td><td>2303.18223</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>text</td><td>LLMs as Recommendation Models. With specific methods or mechanisms, LLMs can be adapted to serve as recom- mendation models. Existing work along this line can be generally divided into two main categories. First, some methods prompt LLMs for completing the recommendation task in a zero-shot paradigm (i.e., without parameter tun- ing) [805, 806]. A series of prompt engineering methods like recency-focused and in-context learning are introduced to improve recommendation performance as well as alleviate the potential model biases [807, 808]. Second, another cat- egory of studies aim to specialize LLMs for personalized recommendation through instruction tuning [357, 809]. Spe- cially, high-quality instruction data is key to adapt LLMs to the recommendation tasks, which can be constructed based on user-item interactions with heuristic templates. To further improve the instruction diversity, InstructRec [357] employs self-instruct technique to simulate large amounts of potential user instructions in various scenarios like product search and personalized recommendations. In addition to representing each item by its text description, there is also growing attention on extending LLMâs vocabulary with semantic identifiers in recommender systems [810, 811], to incorporate collaborative semantics into LLMs.</td></tr>\n",
       "<tr><td>chunk_id</td><td>480.0</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>authors</td><td>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2303.18223</td></tr>\n",
       "<tr><td>summary</td><td>Language is essentially a complex, intricate system of human expressions\n",
       "governed by grammatical rules. It poses a significant challenge to develop\n",
       "capable AI algorithms for comprehending and grasping a language. As a major\n",
       "approach, language modeling has been widely studied for language understanding\n",
       "and generation in the past two decades, evolving from statistical language\n",
       "models to neural language models. Recently, pre-trained language models (PLMs)\n",
       "have been proposed by pre-training Transformer models over large-scale corpora,\n",
       "showing strong capabilities in solving various NLP tasks. Since researchers\n",
       "have found that model scaling can lead to performance improvement, they further\n",
       "study the scaling effect by increasing the model size to an even larger size.\n",
       "Interestingly, when the parameter scale exceeds a certain level, these enlarged\n",
       "language models not only achieve a significant performance improvement but also\n",
       "show some special abilities that are not present in small-scale language\n",
       "models. To discriminate the difference in parameter scale, the research\n",
       "community has coined the term large language models (LLM) for the PLMs of\n",
       "significant size. Recently, the research on LLMs has been largely advanced by\n",
       "both academia and industry, and a remarkable progress is the launch of ChatGPT,\n",
       "which has attracted widespread attention from society. The technical evolution\n",
       "of LLMs has been making an important impact on the entire AI community, which\n",
       "would revolutionize the way how we develop and use AI algorithms. In this\n",
       "survey, we review the recent advances of LLMs by introducing the background,\n",
       "key findings, and mainstream techniques. In particular, we focus on four major\n",
       "aspects of LLMs, namely pre-training, adaptation tuning, utilization, and\n",
       "capacity evaluation. Besides, we also summarize the available resources for\n",
       "developing LLMs and discuss the remaining issues for future directions.</td></tr>\n",
       "<tr><td>published</td><td>20230331</td></tr>\n",
       "<tr><td>title</td><td>A Survey of Large Language Models</td></tr>\n",
       "<tr><td>doc_id</td><td>2303.18223</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section-breakpoint.png](https://i.ibb.co/344JqH3/section-breakpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know the answer to the question about writing side by side summaries between Mistral, Kosmos, and Palm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, context = rag(\n",
    "    \"Write side by side summaries between mistral, kosmos, palm. If there is key difference between them, what is it\",\n",
    "    index,\n",
    "    openai,\n",
    "    top_k=10,\n",
    ")\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "    <style>\n",
       "        .container {\n",
       "            display: flex;\n",
       "            flex-wrap: wrap;\n",
       "        }\n",
       "        .table-container {\n",
       "            margin: 10px;\n",
       "            padding: 10px;\n",
       "            border: 1px solid #dddddd;\n",
       "        }\n",
       "        table {\n",
       "            font-family: arial, sans-serif;\n",
       "            border-collapse: collapse;\n",
       "            width: 100%;\n",
       "        }\n",
       "        td, th {\n",
       "            border: 1px solid #dddddd;\n",
       "            text-align: left;\n",
       "            padding: 8px;\n",
       "        }\n",
       "    </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div class=\"container\">\n",
       "            \n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>12.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
       "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
       "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
       "# Instruction Finetuning</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>9.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
       "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
       "3\n",
       "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>8.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td># 3 Results\n",
       "We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
       "â¢ Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\n",
       "â¢ World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\n",
       "â¢ Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\n",
       "â¢ Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\n",
       "â¢ Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\n",
       "â¢ Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>3.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
       "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
       "# 2 Architectural details\n",
       "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "        <div class=\"table-container\">\n",
       "            <table>\n",
       "                <tr>\n",
       "                    <th>Key</th>\n",
       "                    <th>Value</th>\n",
       "                </tr>\n",
       "                <tr><td>authors</td><td>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</td></tr>\n",
       "<tr><td>chunk_id</td><td>14.0</td></tr>\n",
       "<tr><td>doc_id</td><td>2310.06825</td></tr>\n",
       "<tr><td>primary_category</td><td>cs.CL</td></tr>\n",
       "<tr><td>published</td><td>20231010</td></tr>\n",
       "<tr><td>source</td><td>http://arxiv.org/pdf/2310.06825</td></tr>\n",
       "<tr><td>summary</td><td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\n",
       "for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\n",
       "across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\n",
       "code generation. Our model leverages grouped-query attention (GQA) for faster\n",
       "inference, coupled with sliding window attention (SWA) to effectively handle\n",
       "sequences of arbitrary length with a reduced inference cost. We also provide a\n",
       "model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\n",
       "the Llama 2 13B -- Chat model both on human and automated benchmarks. Our\n",
       "models are released under the Apache 2.0 license.</td></tr>\n",
       "<tr><td>text</td><td>Table 3: Comparison of Chat models. Mistral 7B â Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B â Chat models.\n",
       "In this evaluation, participants were provided with a set of questions along with anonymous responses from two models and were asked to select their preferred response, as illustrated in Figure 6. As of October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.\n",
       "4\n",
       "âe LlaMA2 âe- LLaMA2 65) = Mistral 70; = Mistral a = |. 60; & inal = 268 3 â¬ = 55 8 = Â§ 66 50 Â« Effective LLaMA 64 Effective LlaMA 451 Â¢ i size 23B (3.3x)___ : __size 38B (5.4x)_{ : 7 13 34 70 7 13 34 70 Model size (billion parameters) = Model size (billion parameters) 70) âeâ LLaMA 2 âe- LLaMA2 65) = Mistral Zee} = Mistral FS < 2 60 364, 3 5 2 2 B55 Â£62 Ã© 5 & fa â50 5 2 60 a LlaMA e LLaMA 45 ize 9x) si B (3x fi 13 34 70 7 13 34 70 Model size (billion parameters) Model size (billion parameters)</td></tr>\n",
       "<tr><td>title</td><td>Mistral 7B</td></tr>\n",
       "            </table>\n",
       "        </div>\n",
       "        \n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_retrieved_context(context.matches[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1 - Query Comprehension\n",
    "\n",
    "Your task is to figure out how to execute the following query:  \n",
    "\n",
    "`Write side by side summaries between mistral, kosmos, palm. If there is key difference between them, what is it`\n",
    "\n",
    "Hints:\n",
    "1. JSON response from GPT: https://platform.openai.com/docs/api-reference/chat/create \n",
    "2. Use LLM to extract information from the query\n",
    "3. Apply the right techniques of RAG to construct the best response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a query comprehension\n",
    "\n",
    "# 1. Use LLM to extract the following information:\n",
    "#     - Category - (summarization, comparison, other, etc.)\n",
    "#     - Queries to run based on the category\n",
    "# 2. IF the category is \"summarization\":\n",
    "#    - run retrieval on query to get the context and ask llm to summarize the context\n",
    "# 3. IF the category is \"other\":\n",
    "#    - run the rag function pipeline\n",
    "# 4. IF the category is \"comparison\":\n",
    "#    - run multiple retrieval + summarization pipelines and ask llm to compare them\n",
    "\n",
    "# Example 1: user_query = \"Write side by side summaries between mistral, kosmos, palm. If there is key difference between them, what is it\"\n",
    "# 1. LLM will return the category as \"comparison\", and queries to run as [\"mistral\", \"kosmos\", \"palm\"]\n",
    "# 2. Run the retrieval + summarization pipeline for each query\n",
    "# 3. Ask llm to compare the summaries, write it side by side and highlight the key differences\n",
    "\n",
    "# Example 2: user_query = \"What are key benefits of Mistral 7B?\"\n",
    "# 1. LLM will return the category as \"summarization\", and queries to run as [\"Key benefits of Mistral 7B\"]\n",
    "# 2. Run the retrieval + summarization pipeline for the query\n",
    "\n",
    "# Example 3: user_query = \"How to fine tune LLAMA 2?\"\n",
    "# 1. LLM will return the category as \"other\", and queries to run as [\"How to fine tune LLAMA 2\"]\n",
    "# 2. Run the rag pipeline for the query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2 - Paper listing and filtering\n",
    "\n",
    "Adapt RAG model to work on paper listing, ie it should be able to answer this type of question:  \n",
    "`List me papers names and sources that are published in 2023 about e-commerce search techniques`\n",
    "\n",
    "Hints:\n",
    "1. Use metadata filtering: https://docs.pinecone.io/guides/data/filter-with-metadata \n",
    "2. Use LLM (prompt engineering) to extract the useful data from the query (search term, date, task, etc)\n",
    "3. JSON reponse from ChatGPT - https://platform.openai.com/docs/api-reference/chat/create   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Once we explored the simple components of RAG, we can use existing frameworks to build them.  \n",
    "\n",
    "Langchain: https://www.langchain.com/langchain \n",
    "\n",
    "```python\n",
    "!pip install -qU \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index, embedding=embeddings, text_key=\"text\", pinecone_api_key=PINECONE_API_KEY, index_name=INDEX_NAME,)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "langchain_rag = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "langchain_rag.invoke(\"What are all benefits of using Mistral 7B?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune vs RAG vs Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag-vs-prompt-vs-finetune.png](https://i.ibb.co/C8rvnTY/Screenshot-2024-05-30-at-21-17-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual-breakpoint.png](https://i.ibb.co/rHVSp3w/visual-breakpoint.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
